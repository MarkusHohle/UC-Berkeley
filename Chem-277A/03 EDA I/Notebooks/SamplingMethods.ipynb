{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e6eb1b-44fd-4495-9ffb-192ee13d4414",
   "metadata": {},
   "source": [
    "# Data Sampling - Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486af501-107a-4cce-9b26-7c951a75a386",
   "metadata": {},
   "source": [
    "## 1) Objective\n",
    "\n",
    "In the last examples, we dealed with larger than memory files. Another time and cost efficient way of exploring a large data set is sampling. The idea is that we don't need the entire data set in order to analyze it. We pick specific portions via sampling. Depending on the tasks, there are different sampling methods:<br>\n",
    "<br>\n",
    "- Random Sampling  \n",
    "- Stratified Sampling  \n",
    "- Systematic Sampling  \n",
    "- Cluster Sampling  \n",
    "- Bootstrap Sampling  \n",
    "- Oversampling & Undersampling (Basic Concepts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15502eb1-80e8-4878-a934-c639843d5b29",
   "metadata": {},
   "source": [
    "## 2) Preperation\n",
    "First, we load the standard python libraries as ususal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987ebfc-64a8-4113-a9e9-9572e0c0791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                    # for unix like commands like \"listdir\" etc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6809d34e-8ba6-47b3-b57e-c8f91673b44b",
   "metadata": {},
   "source": [
    "We are going to need a few more other methods that help us to determine the quality of the sample (see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743419d2-b376-4b4e-9b60-2bb1f73b64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde, entropy, ks_2samp     # gaussian_kde: for plotting a smoothed histogram\n",
    "                                                            # entropy: for calculating the KL-divergence\n",
    "                                                            # ks_2samp: for running a KS-test\n",
    "from sklearn.model_selection import train_test_split        # for stratified sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15965409-92d9-4744-b690-4eac3969769a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0496893-0dbf-4bfc-ab77-2a77305e62b2",
   "metadata": {},
   "source": [
    "We want to explore the data set *\"DTXSID8031865 HTTr-Summary-2025-10-07.xlsx\"* that contains chemical properies and experimental results of so called PFAS: **P**er- and poly**F**luoro**A**lkyl **S**ubstances which are of particular health and environmental intrest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e85a6f-e129-4ebe-a334-fe66dbb4e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"DTXSID8031865 HTTr-Summary-2025-10-07.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac92d14-012d-4b39-ab36-05f1dc9f2114",
   "metadata": {},
   "source": [
    "In order to locate the file we run *\"FindMyFile\"* as before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfcb11-0eec-4b38-a137-4061057f7504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindMyFile(filename: str, ServerHardDiscPath: str = r\"c:\\Users\\MMH_user\\Desktop\") -> str:\n",
    "    \"\"\"\n",
    "    finds file of name \"filename\" anywhere in \"ServerHardDiscPath\" and returns complete path\n",
    "    \"\"\"\n",
    "    for r,d,f in os.walk(ServerHardDiscPath):\n",
    "        for files in f:\n",
    "             if files == filename: #for example \"MessyFile.xlsx\"\n",
    "                 file_name_and_path =  os.path.join(r,files)\n",
    "                 return file_name_and_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae2d88f-76a7-4beb-880b-a37a81cae381",
   "metadata": {},
   "outputs": [],
   "source": [
    "File = pd.read_excel(FindMyFile(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd15e46-2400-4852-8987-98ce6ee110d8",
   "metadata": {},
   "source": [
    "...and explore the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2097c8-42f3-44a5-8e30-78cba14dad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "File.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5f90a3-dcb4-458e-8ae6-d97beef08bdf",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bea97c",
   "metadata": {},
   "source": [
    "## 3) Random Sampling\n",
    "The most straight forward way is just random sampling. Say, we want to sample the information stored in column \"BMD\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e23b01-29c4-44bb-baf3-e01735fccfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'BMD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a8813",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_random = File[col].sample(n = 10)\n",
    "sample_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7187c40c-c61f-4ac7-bc84-92cdff7b1317",
   "metadata": {},
   "source": [
    "How representative the subsample (here n=10) is mainly depends on the subsample size and also on the total size of the data set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfad587f-190a-4cd5-8034-d40b934342d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N total = \" + str(len(File[col])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98d8a80-6851-4ed0-a49d-34d290d6a10e",
   "metadata": {},
   "source": [
    "We want to draw subsamples of different sizes, say 1%, 5% etc and compare the subsample distribution to the distribution of the complete dataset in order to get an idea how representative the subsample is in this case. In order to measure \"representativeness\" we generate a plot (subsample vs all data), but also calculate the **KL-divergence** (see lecture), which is entropy based and tells us how much information we loose if we sample the data with different *n*. The higher the **KL-divergence** the more the subsample differs from the actual data and the more information we lost.<br>  \n",
    "Another method to measure how similar distributions are is the **K**olmogorov-**S**mirnov (KS)-test. The p-value from the KS-test equals the probability to have the measured (or more extreme) discripancy between the two data sets assuming that we drew them from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9079a-48db-4aba-9c99-6a846bed7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sizes = [1, 5, 10, 20, 50, 90]# in %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f22c59-e28d-4a80-a85b-12fb11670ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RandomSampleData(df, col: str, sample_sizes: list):\n",
    "\n",
    "    # Full data distribution\n",
    "    full_data = df[col].values\n",
    "    kde_full  = gaussian_kde(full_data)\n",
    "    n_total   = len(full_data)\n",
    "    LS        = len(sample_sizes)\n",
    "    \n",
    "    # Shared x-axis for KDE plots\n",
    "    x_vals       = np.linspace(full_data.min(), full_data.max(), 500)\n",
    "    full_density = kde_full(x_vals)\n",
    "    \n",
    "    # Prepare subplots\n",
    "    fig, axes = plt.subplots(2, int(np.ceil(LS/2)), figsize = (18, 10))\n",
    "    axes      = axes.flatten()\n",
    "    \n",
    "    results   = [None]*LS\n",
    "    \n",
    "    for i, (ax, pct) in enumerate(zip(axes, sample_sizes)):\n",
    "\n",
    "        n_sample = max(1, int(n_total * pct / 100))\n",
    "        \n",
    "        # Vectorized random sampling\n",
    "        sample = df[col].sample(n_sample, replace=False).values\n",
    "        # KDE of sample for plotting a smooth histogram\n",
    "        kde_sample     = gaussian_kde(sample)\n",
    "        sample_density = kde_sample(x_vals)\n",
    "    \n",
    "        # KL divergence (add small epsilon for numerical stability)\n",
    "        KL = entropy(sample_density + 1e-12, full_density + 1e-12)\n",
    "    \n",
    "        # KS test\n",
    "        KS = ks_2samp(sample, full_data).pvalue\n",
    "    \n",
    "        results[i] = (pct, KL, KS)\n",
    "    \n",
    "        # Plot\n",
    "        ax.plot(x_vals, full_density, label=\"Full Data\", linewidth=2.5, alpha=0.8)\n",
    "        ax.plot(x_vals, sample_density, label=f\"{pct}% sample\", linewidth=2)\n",
    "        ax.set_xlabel(col)\n",
    "        ax.set_ylabel('rel. frequency')\n",
    "        ax.set_title(f\"Sample Size: {pct}%\\nKL = {KL:.4f}, KS p-val = {KS:.4f}\")\n",
    "        ax.grid(alpha=0.2)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show comparison table\n",
    "    pd.DataFrame(results, columns=[\"Sample %\", \"KL Divergence\", \"KS Statistic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca16c5-e636-40a3-9334-9898fc677ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomSampleData(File, col, sample_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fde436-a04d-4ae6-854b-2a93e3591028",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ffdcf2",
   "metadata": {},
   "source": [
    "## 4) Stratified Sampling\n",
    "Random sampling does not take into account that the data might have been drawn from different groups of different sizes. **Stratified sampling maintains the proportion of groups (strata)**. Note that random sampling results in **approximate proportions only on average across many repeated samples, not in any single sample,** especially if the different proportions cover a large dynamic range.<br>\n",
    "For example the groups in *'TARGET_LEVEL'* have completely different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b46e2f6-4e99-4c2a-b88f-a5cd846bd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_groups = 'TARGET_LEVEL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477ef286-14cd-4cac-81c9-be2cdc46c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "Groups     = set(File[col_groups])\n",
    "print(Groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe35cc3f-45a5-4dcd-a24c-1c4e81ed32c7",
   "metadata": {},
   "source": [
    "Get sizes by counting appearance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051749c7-3164-49e1-bf84-a200b37361a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = list(File[col_groups])\n",
    "for g in Groups:\n",
    "    ct = L.count(g)\n",
    "    print(g + \": \" + str(ct) + \" appearances\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f57282-025a-437a-a234-8b22d231a68e",
   "metadata": {},
   "source": [
    "Therefore, we need to sample the groups separately. The **subsamples will have different absolute sizes, but identical relative sizes.**<br>\n",
    "Say, we want to extract 20% of each group, we can take advantage over the method *\"train_test_split\"*, but only work with the training sample which will be the subsamples we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5322c-bb75-45a4-9871-895bae5346c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = 0.2\n",
    "_, test  = train_test_split(File, test_size = fraction, stratify = File[col_groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf57b09-b5d2-43a1-ab91-ea077db90b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[[col_groups, col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01fec25-3ba8-436a-926e-86cb083c66f9",
   "metadata": {},
   "source": [
    "Let us check if that worked! The subsamples should be roughly *\"fraction\"* of the size of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cf07d9-6f89-45b3-8746-cfc08a6a2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ltest = list(test[col_groups])\n",
    "for g in Groups:\n",
    "    ct     = L.count(g)\n",
    "    cttest = Ltest.count(g)\n",
    "    r      = cttest/ct\n",
    "    print(g + \":\\t\\t \" + str(ct) + \" appearances in full data set,\\t \" + str(cttest) + \" appearances in subsample.\\t Ratio = \" + f\"{r: .2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15697ce3-cfc0-4b76-a451-76ffd66eae08",
   "metadata": {},
   "source": [
    "Therefore, let us plot the smoothed histograms of the values of the different groups.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31994f59-7d76-486a-a45b-c37ae58bd0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotSampleData(df, Groups, col_groups, col):\n",
    "\n",
    "    for g in Groups:\n",
    "        #extracting all(!) values of each group\n",
    "        vals = np.array([df[col].iloc[i] for i, gcol in enumerate(df[col_groups]) if gcol == g])\n",
    "        n    = len(vals)\n",
    "        \n",
    "        if n>1:#in case there is only one data point\n",
    "            x_vals         = np.linspace(vals.min(), vals.max(), 500)\n",
    "        \n",
    "            kde_sample     = gaussian_kde(vals)\n",
    "            sample_density = kde_sample(x_vals)\n",
    "    \n",
    "            #normalization for plotting\n",
    "            sample_density /= np.sum(sample_density)\n",
    "        \n",
    "            plt.plot(x_vals, sample_density, linewidth = 2, label  = g + \": n = \" + str(n))\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('norm. frequency')\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "            plt.grid(alpha = 0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1907a-66ed-4c1c-acb7-45c1af821077",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotSampleData(File, Groups, col_groups, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3051048-69ba-4ed8-8baa-0f41d18120d0",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cfe58e",
   "metadata": {},
   "source": [
    "## 5) Systematic Sampling\n",
    "Systematic sampling refers to sampling only **each k-th element** and is easy to implement. It ensures the sample is evenly distributed across the population. However, systematic sampling **should be avoided if there are pattern** expected to occur in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8a9ea-6604-4e90-844a-55acd8c70082",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "systematic_sample = File[col].iloc[::k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1718284-6d83-4071-bba4-c0ace5a05bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "systematic_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322bddc-90cf-47af-abf0-61776590b8a6",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e91aa41",
   "metadata": {},
   "source": [
    "## 6) Cluster Sampling\n",
    "For cluster sampling, we randomly select a **ceratin number of clusters (groups)** and then **select all members from these selected clusters**, whereas  for stratiefied sampling we selected all clusters (groups) and then sampled some members from those clusters (groups)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b475f80-2b62-40da-8d9c-ef12e42777ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nClust            = 3\n",
    "selected_clusters = np.random.choice(File[col_groups].unique(), size = nClust, replace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c2672e-3da0-40c2-aa0c-3d1db27c57bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634529a4-06c1-46dd-88a7-d063aa67fbb1",
   "metadata": {},
   "source": [
    "Extracting all members from the randomly selected cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bcf0ca-8c03-443e-886e-0a7ed6f1dfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sample = File[File[col_groups].isin(selected_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d081fe1-d4e6-4ce2-8faf-f78f1080287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_sample[[col_groups, col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a08886-0e49-436b-ab4c-f4fb82817451",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotSampleData(File, selected_clusters, col_groups, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb171b0b-629f-4110-b602-6b0c83d77caa",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5190554",
   "metadata": {},
   "source": [
    "## 7) Bootstrap Sampling\n",
    "Bootstrap sampling is essentially **sampling with replacement** and often used for **estimating uncertainty**.<br>\n",
    "When we randomly sample a certain number *n* from our column \"BMT\", we found in *3)* that this imposes uncertainty about the actual distribution of the sample. In order to estimate this uncertainy, we sample our *n* values with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a5f3f-f9ae-4530-af9a-31d459127fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906dec7f-54ee-48e6-9c40-be607b3ad341",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdc2193-4c1d-485e-b6fd-051beeb1fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_random = File[col].sample(n = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e073831c-cbc3-456b-aa57-07edcf649452",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"N total = \" + str(len(File[col]))) # size of the actual data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620bb8b-0358-457d-b845-6c1365f8f06d",
   "metadata": {},
   "source": [
    "Comparing sample to actual distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad51eeb-16cb-481b-9be5-57423c239097",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = File[col].values\n",
    "kde_full  = gaussian_kde(full_data)\n",
    "    \n",
    "# Shared x-axis for KDE plots\n",
    "x_vals         = np.linspace(full_data.min(), full_data.max(), 500)\n",
    "full_density   = kde_full(x_vals)\n",
    "kde_sample     = gaussian_kde(sample_random)\n",
    "sample_density = kde_sample(x_vals)\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_vals, full_density, label = \"Full Data (size \" + str(len(File[col])) + \")\", linewidth = 2.5, alpha = 0.8)\n",
    "plt.plot(x_vals, sample_density, label = \"Sample (size \" + str(n) + \")\", linewidth = 2)\n",
    "plt.xlabel(col)\n",
    "plt.ylabel('norm. frequency')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.grid(alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e4581-463a-4392-b543-8d9a8dec1a2a",
   "metadata": {},
   "source": [
    "Let us now bootstrap the sample many times, i. e. we \"return\" it to the overall dataset and then draw *n* data points again, save the result, put the datapoints back and so on. Some of the datapoints will be drawn many times during this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7872227-e3fd-4bc6-ac5d-5f567e493977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nboot = 500 #number of boot strapping steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a518b76-b35e-4c49-8855-227df521f658",
   "metadata": {},
   "source": [
    "For picking random integer numbers in a certain intervall without repetition, we call the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4916aa-1c55-49be-bea2-2e2932e70eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e422cef4-1767-4058-befe-f28628262002",
   "metadata": {},
   "source": [
    "Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0701abb-515b-41b6-aa00-79e7377a4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nmin = 10\n",
    "Nmax = 20\n",
    "Nnum = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e0ca86-7461-4a1e-9b7b-800200e21edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(random.sample(range(Nmin, Nmax + 1), Nnum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcbbba9-7dc2-4b80-a14b-2fb0e36d757c",
   "metadata": {},
   "source": [
    "We will use this for indexing in the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea1ca5c-fb91-459d-bdcb-be649d2bfbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BootStrap(File, col, Size_sample: int = 50, Nboot: int = 500):\n",
    "    \n",
    "    L         = len(File[col])\n",
    "    Boot_data = np.zeros((Nboot, Size_sample))    #pre-allocating space\n",
    "\n",
    "\n",
    "    for i in range(Nboot):\n",
    "        idx          = random.sample(range(0, L), Size_sample) # drawing Size_sample random numbers between 0 and L\n",
    "        Boot_data[i] = File[col].iloc[idx] \n",
    "\n",
    "    return Boot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22a3036-5d4d-4488-a5b5-984df769b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boot_data = BootStrap(File, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1045fbb9-d2aa-4fe2-872a-91bc423328ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Boot_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d09aa3-f1bb-42df-8540-17004020278c",
   "metadata": {},
   "source": [
    "In order to understand how we can estimate uncerainty, we plot all the smoothed histograms in one figure and see who the curves vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6815e4d1-7661-4a34-85c4-d0d0203318a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = File[col].values\n",
    "x_vals    = np.linspace(full_data.min(), full_data.max(), 500)    \n",
    "\n",
    "full_density   = kde_full(x_vals)\n",
    "sample_density = kde_sample(x_vals)\n",
    "\n",
    "#smooth histograms\n",
    "for i in range(Nboot):\n",
    "    boot       = Boot_data[i,:]\n",
    "    kde_sample = gaussian_kde(boot)\n",
    "    y          = kde_sample(x_vals)\n",
    "    \n",
    "    plt.plot(x_vals, y, c = 'black', linewidth = 2.5, alpha = 0.01)\n",
    "plt.plot(x_vals, full_density, c = 'red', linewidth = 1, alpha = 0.8, label = \"Full Data (size \" + str(len(File[col])) + \")\")    \n",
    "plt.xlabel(col)\n",
    "plt.ylabel('norm. frequency')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.grid(alpha = 0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411b445a-74cb-492d-9051-895a031d78d5",
   "metadata": {},
   "source": [
    "We can clearly see how the sample curves fluctuate. Based on these fluctuations, it is possible to calculate percentiles and interprete them as confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55e4b4-85cd-4856-bb4f-4d2ea28cca51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotConfIntervall(File, col, Boot_data, Conf: list = [68, 90, 95, 99]):\n",
    "\n",
    "    full_data      = File[col].values\n",
    "    x_vals         = np.linspace(full_data.min(), full_data.max(), 500)    \n",
    "\n",
    "    full_density   = kde_full(x_vals)\n",
    "\n",
    "    Y              = np.zeros((Boot_data.shape[0], x_vals.shape[0])) #pre-allocating space\n",
    "\n",
    "    #smooth histograms\n",
    "    for i in range(Nboot):\n",
    "        boot       = Boot_data[i,:]\n",
    "        kde_sample = gaussian_kde(boot)\n",
    "        Y[i,:]     = kde_sample(x_vals)\n",
    "\n",
    "    #calculating confidence intervals using np.percentile\n",
    "    for v in Conf:\n",
    "        alpha      = v/100\n",
    "        y_ci_lower = np.percentile(Y, (1 - alpha) / 2 * 100, axis = 0)\n",
    "        y_ci_upper = np.percentile(Y, (1 + alpha) / 2 * 100, axis = 0)\n",
    "        \n",
    "        plt.fill_between(x_vals, y_ci_lower, y_ci_upper, color = 'black', alpha = 0.1, label = str(v) + '% Confidence Interval')\n",
    "    plt.plot(x_vals, full_density, c = 'red', linewidth = 1, alpha = 0.8, label = \"Full Data (size \" + str(len(File[col])) + \")\")    \n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('norm. frequency')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plt.grid(alpha = 0.2)\n",
    "    plt.title('bootstrapped sample')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d17441e-8166-400c-9ec6-fa5a4527c235",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotConfIntervall(File, col, Boot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3cd94-29ee-430e-a0c3-95b62d0a3a81",
   "metadata": {},
   "source": [
    "Thanks to the percentiles, it is possible to quantify the uncertainty of the sample values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb8d784-80f0-49b9-91f2-fe41848c7930",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651f493e",
   "metadata": {},
   "source": [
    "## 8) Oversampling & Undersampling\n",
    "Imagine you like to train an ANN on images of cats and dogs. Ideally, the training data is well balanced, say we have 10,000 cat images and 10,000 dog images. Once the ANN is trained, one way to estimate the quality of the classification is calculating the accuracy: how often did the network classify the images correctly.<br>\n",
    "Unfortunately, oftentimes the training **sample is not well balanced**. Imagine the extreme, when we have 10,000 dog images but only 1,000 cat images. If the network always votes for *\"dog\"* regardless of the actual image, the accuracy will be close to 90%! Apparantly it is impossible for a classifier to learn the classes when we have such a stark inbalance. The problem can be solved with oversampling and undersampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b27b21-82cd-4b14-8c6a-010a95eaa05f",
   "metadata": {},
   "source": [
    "First, we create an inbalanced sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc25cba-2296-4d13-bf70-7b616f6ab480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'x': np.random.randn(200), #randon numbers stand symbolically for images\n",
    "    'y': np.random.choice(['dog', 'cat'], size=200, p=[0.9, 0.1])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3c8946-8f1b-4dc2-b817-680390c6ca20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76707a58-6581-4bcf-a007-08010d4cfda2",
   "metadata": {},
   "source": [
    "First, we need to determine the size of the different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839f147a-5f7e-4e1c-bc32-eeebd1345eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ndog = len(df[df['y']=='dog'])\n",
    "Ncat = len(df[df['y']=='cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d829b7e-1866-436f-a562-c4190e6b49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ndog, Ncat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cbea2-1539-4ca4-97a4-5662bd8545fb",
   "metadata": {},
   "source": [
    "Since the sample *\"dog\"* is overrepresented, it is called *\"majority\"* and the underrepresented sample is called *\"minority\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9283acc4-d0a2-4a41-9ca8-3c83b62a4788",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = df[df['y']=='dog']\n",
    "minority = df[df['y']=='cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f82a9e3-8536-456c-95c7-ca463b2015a2",
   "metadata": {},
   "source": [
    "We can either **oversample the minority**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6b393-1967-4ec9-a327-c6d405467b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_oversampled = minority.sample(Ndog, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c7ea4-b74d-448b-ae53-75e9834bdebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7343ff2-6933-495e-9d5f-aba6c0036c29",
   "metadata": {},
   "source": [
    "or **undersample the majority**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1330780-d8bf-4475-887d-607c0c2c7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_undersampled = majority.sample(Ncat, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44ea64-319d-46b0-801e-59976716ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_undersampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd044d7-e620-40aa-a532-c51383809d3b",
   "metadata": {},
   "source": [
    "in order to balance the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
