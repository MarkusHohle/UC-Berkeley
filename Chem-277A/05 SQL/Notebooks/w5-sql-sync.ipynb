{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8010a5",
   "metadata": {},
   "source": [
    "# Week 5 SQL: End-to-End Data Analysis\n",
    "\n",
    "## Session Overview\n",
    "\n",
    "In this  session, we will work through a data analysis workflow combining SQL and Pandas. Our goal is to analyze experimental data from a drug discovery program and identify the most promising compound-target combinations for further development.\n",
    "\n",
    "We will simulate a scenario where you are a data scientist at a pharmaceutical company. Suppose you have access to a large database of experimental results, and your task is to prepare a report identifying promising drug candidates. The workflow will include data filtering with SQL, quality control checks, exploratory data analysis with Pandas, and visualization of results.\n",
    "\n",
    "By the end of this session, you will have completed a full analysis pipeline that demonstrates when to use SQL versus Pandas and how to combine them effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff94c91",
   "metadata": {},
   "source": [
    "## The Business Problem\n",
    "\n",
    "Your research team has been screening a library of chemical compounds against various protein targets associated with cancer. After months of experiments, you have accumulated:\n",
    "\n",
    "- 500 chemical compounds with known properties\n",
    "- 100 protein targets from different families\n",
    "- Over 2000 experimental assay results measuring compound-protein binding\n",
    "\n",
    "The project manager has asked you to identify promising candidates for the next phase of drug development. Specifically, you need to:\n",
    "\n",
    "1. Filter for drug-like compounds that meet Lipinski's Rule of Five criteria\n",
    "2. Identify compounds with strong and consistent binding (low IC50 values with low variability)\n",
    "3. Focus on kinase targets, which are particularly relevant for cancer therapy\n",
    "4. Perform quality control to ensure reliable results\n",
    "5. Rank candidates and provide visualizations for the team meeting\n",
    "\n",
    "Let's begin by setting up our environment and loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c19873",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create DuckDB connection\n",
    "con = duckdb.connect(':memory:')\n",
    "\n",
    "print(\"Environment ready. DuckDB version:\", duckdb.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee506da",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect the Data\n",
    "\n",
    "In a real scenario, this data would come from a database or data warehouse. For our exercise, we will generate synthetic data. In practice, you would connect to your database using connection strings, but DuckDB makes it easy to work with both databases and in-memory data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e97607",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Compounds library (synthetic)\n",
    "n_compounds = 500\n",
    "compounds_data = {\n",
    "    'compound_id': [f'CMPD_{i:04d}' for i in range(1, n_compounds + 1)],\n",
    "    'compound_name': [f'MolSci-{i}' for i in range(1, n_compounds + 1)],\n",
    "    'molecular_weight': np.random.uniform(150, 800, n_compounds).round(2),\n",
    "    'log_p': np.random.uniform(-3, 7, n_compounds).round(2),\n",
    "    'h_bond_donors': np.random.randint(0, 8, n_compounds),\n",
    "    'h_bond_acceptors': np.random.randint(0, 12, n_compounds),\n",
    "    'synthesis_date': [datetime(2023, 1, 1) + timedelta(days=int(x)) \n",
    "                       for x in np.random.uniform(0, 700, n_compounds)],\n",
    "    'batch_quality': np.random.choice(['Excellent', 'Good', 'Fair'], n_compounds, p=[0.6, 0.3, 0.1])\n",
    "}\n",
    "compounds_df = pd.DataFrame(compounds_data)\n",
    "\n",
    "# Protein targets (synthetic)\n",
    "n_proteins = 100\n",
    "protein_families = ['Kinase', 'GPCR', 'Ion Channel', 'Protease', 'Nuclear Receptor', 'Phosphatase']\n",
    "proteins_data = {\n",
    "    'protein_id': [f'PROT_{i:03d}' for i in range(1, n_proteins + 1)],\n",
    "    'protein_name': [f'Target-{i}' for i in range(1, n_proteins + 1)],\n",
    "    'protein_family': np.random.choice(protein_families, n_proteins),\n",
    "    'disease_relevance': np.random.choice(['Cancer', 'Inflammation', 'Metabolic', 'Neurological'], \n",
    "                                          n_proteins, p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'validation_status': np.random.choice(['Validated', 'Exploratory'], n_proteins, p=[0.7, 0.3])\n",
    "}\n",
    "proteins_df = pd.DataFrame(proteins_data)\n",
    "\n",
    "# Assay results with sampled patterns\n",
    "n_results = 2000\n",
    "assay_results_data = {\n",
    "    'assay_id': [f'ASSAY_{i:05d}' for i in range(1, n_results + 1)],\n",
    "    'compound_id': np.random.choice(compounds_df['compound_id'].values, n_results),\n",
    "    'protein_id': np.random.choice(proteins_df['protein_id'].values, n_results),\n",
    "    'ic50_nm': np.random.lognormal(mean=4, sigma=2.5, size=n_results).round(2),\n",
    "    'efficacy_percent': np.random.uniform(20, 100, n_results).round(1),\n",
    "    'assay_date': [datetime(2024, 1, 1) + timedelta(days=int(x)) \n",
    "                   for x in np.random.uniform(0, 330, n_results)],\n",
    "    'lab_technician': np.random.choice(['Alice Chen', 'Bob Kumar', 'Carol Martinez', 'David Wong'], n_results),\n",
    "    'replicate_number': np.random.randint(1, 4, n_results)\n",
    "}\n",
    "assay_results_df = pd.DataFrame(assay_results_data)\n",
    "\n",
    "# We add some intentional anomalies to demonstrate quality control as a use case\n",
    "anomaly_indices = np.random.choice(n_results, size=20, replace=False)\n",
    "assay_results_df.loc[anomaly_indices, 'ic50_nm'] *= 10  # Some spurious high values\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"\\nCompounds: {len(compounds_df):,} records\")\n",
    "print(f\"Proteins: {len(proteins_df):,} records\")\n",
    "print(f\"Assay Results: {len(assay_results_df):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e86833",
   "metadata": {},
   "source": [
    "## Step 2: Initial Data Exploration with SQL\n",
    "\n",
    "Before diving into complex analysis, we should understand the basic characteristics of our data. Let's use SQL to quickly get summary statistics. This is often faster than Pandas for large datasets, and the queries are self-documenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d297733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for compounds\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_compounds,\n",
    "        AVG(molecular_weight) as avg_mw,\n",
    "        MIN(molecular_weight) as min_mw,\n",
    "        MAX(molecular_weight) as max_mw,\n",
    "        AVG(log_p) as avg_logp\n",
    "    FROM compounds_df\n",
    "\"\"\"\n",
    "\n",
    "print(\"Compound Library Summary:\")\n",
    "print(con.execute(query).df())\n",
    "\n",
    "# Distribution of compounds by batch quality\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        batch_quality,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(100.0 * COUNT(*) / SUM(COUNT(*)) OVER (), 1) as percentage\n",
    "    FROM compounds_df\n",
    "    GROUP BY batch_quality\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\nBatch Quality Distribution:\")\n",
    "print(con.execute(query).df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4272b25",
   "metadata": {},
   "source": [
    "Now let's look at the protein target distribution and the overall assay statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f5660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Protein target distribution\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        protein_family,\n",
    "        disease_relevance,\n",
    "        COUNT(*) as num_proteins\n",
    "    FROM proteins_df\n",
    "    GROUP BY protein_family, disease_relevance\n",
    "    ORDER BY protein_family, num_proteins DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"Protein Target Distribution:\")\n",
    "print(con.execute(query).df())\n",
    "\n",
    "# Overall assay statistics\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT compound_id) as compounds_tested,\n",
    "        COUNT(DISTINCT protein_id) as proteins_tested,\n",
    "        COUNT(*) as total_assays,\n",
    "        ROUND(AVG(ic50_nm), 2) as mean_ic50,\n",
    "        ROUND(MEDIAN(ic50_nm), 2) as median_ic50,\n",
    "        ROUND(STDDEV(ic50_nm), 2) as std_ic50\n",
    "    FROM assay_results_df\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\\nAssay Results Summary:\")\n",
    "print(con.execute(query).df())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb14280",
   "metadata": {},
   "source": [
    "Notice that the mean IC50 is much higher than the median. This suggests a right-skewed distribution, which is typical for biological activity data. A few inactive compounds with very high IC50 values pull the mean upward. This is why we often use the median or work with log-transformed IC50 values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94857bb0",
   "metadata": {},
   "source": [
    "## Step 3: Filter for Drug-Like Compounds Using SQL\n",
    "\n",
    "Lipinski's Rule of Five provides guidelines for drug-likeness based on molecular properties. Compounds should have molecular weight under 500 Da, log P under 5, fewer than 5 hydrogen bond donors, and fewer than 10 hydrogen bond acceptors (exceptions to more than one of these criteria breaks the Ro5). Let's use SQL to filter our compound library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lipinski's Rule of Five\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        compound_id,\n",
    "        compound_name,\n",
    "        molecular_weight,\n",
    "        log_p,\n",
    "        h_bond_donors,\n",
    "        h_bond_acceptors,\n",
    "        batch_quality,\n",
    "        CASE \n",
    "            WHEN molecular_weight <= 500 \n",
    "                AND log_p <= 5 \n",
    "                AND h_bond_donors <= 5 \n",
    "                AND h_bond_acceptors <= 10 \n",
    "            THEN 'Pass'\n",
    "            ELSE 'Fail'\n",
    "        END as lipinski_status\n",
    "    FROM compounds_df\n",
    "    WHERE batch_quality IN ('Excellent', 'Good')\n",
    "\"\"\"\n",
    "\n",
    "druglike_compounds = con.execute(query).df()\n",
    "\n",
    "print(f\"Total compounds after quality filter: {len(druglike_compounds):,}\")\n",
    "print(f\"Drug-like compounds (pass Lipinski): {(druglike_compounds['lipinski_status'] == 'Pass').sum():,}\")\n",
    "print(f\"Percentage passing: {100 * (druglike_compounds['lipinski_status'] == 'Pass').sum() / len(druglike_compounds):.1f}%\")\n",
    "\n",
    "print(\"\\nSample of drug-like compounds:\")\n",
    "print(druglike_compounds[druglike_compounds['lipinski_status'] == 'Pass'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a6a93",
   "metadata": {},
   "source": [
    "By filtering at the SQL level, we reduce the amount of data we need to work with in subsequent steps. This is a key principle: use SQL to filter large datasets early in your pipeline, then bring the smaller result set into Pandas for detailed analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c852d5",
   "metadata": {},
   "source": [
    "## Step 4: Quality Control - Identifying Reliable Measurements\n",
    "\n",
    "Before analyzing binding data, we need to ensure data quality. Let's look for potential issues such as compounds or proteins with high measurement variability and outlier values that might indicate experimental errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9ceaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for compounds with high variability in measurements\n",
    "query = \"\"\"\n",
    "    WITH compound_variability AS (\n",
    "        SELECT \n",
    "            compound_id,\n",
    "            COUNT(*) as num_measurements,\n",
    "            AVG(ic50_nm) as mean_ic50,\n",
    "            STDDEV(ic50_nm) as std_ic50,\n",
    "            MIN(ic50_nm) as min_ic50,\n",
    "            MAX(ic50_nm) as max_ic50\n",
    "        FROM assay_results_df\n",
    "        GROUP BY compound_id\n",
    "        HAVING COUNT(*) >= 3\n",
    "    )\n",
    "    SELECT \n",
    "        compound_id,\n",
    "        num_measurements,\n",
    "        ROUND(mean_ic50, 2) as mean_ic50,\n",
    "        ROUND(std_ic50, 2) as std_ic50,\n",
    "        ROUND(std_ic50 / mean_ic50, 2) as coefficient_of_variation,\n",
    "        ROUND(min_ic50, 2) as min_ic50,\n",
    "        ROUND(max_ic50, 2) as max_ic50\n",
    "    FROM compound_variability\n",
    "    WHERE std_ic50 / mean_ic50 > 1.0  -- High coefficient of variation\n",
    "    ORDER BY std_ic50 / mean_ic50 DESC\n",
    "    LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "high_variability = con.execute(query).df()\n",
    "print(\"Compounds with high measurement variability (CV > 1.0):\")\n",
    "print(high_variability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b36d834",
   "metadata": {},
   "source": [
    "The coefficient of variation (CV) is the standard deviation divided by the mean. A CV greater than 1.0 indicates that the standard deviation is larger than the mean, suggesting inconsistent measurements. These compounds may have quality issues such as poor solubility, degradation, or assay interference.\n",
    "\n",
    "For our analysis, we will focus on compounds with reliable, reproducible measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfe58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outlier measurements using statistical approach\n",
    "query = \"\"\"\n",
    "    WITH compound_protein_stats AS (\n",
    "        SELECT \n",
    "            compound_id,\n",
    "            protein_id,\n",
    "            AVG(ic50_nm) as mean_ic50,\n",
    "            STDDEV(ic50_nm) as std_ic50,\n",
    "            COUNT(*) as num_replicates\n",
    "        FROM assay_results_df\n",
    "        GROUP BY compound_id, protein_id\n",
    "        HAVING COUNT(*) >= 2\n",
    "    )\n",
    "    SELECT \n",
    "        a.assay_id,\n",
    "        a.compound_id,\n",
    "        a.protein_id,\n",
    "        a.ic50_nm,\n",
    "        s.mean_ic50,\n",
    "        s.std_ic50,\n",
    "        ROUND((a.ic50_nm - s.mean_ic50) / s.std_ic50, 2) as z_score\n",
    "    FROM assay_results_df a\n",
    "    INNER JOIN compound_protein_stats s\n",
    "        ON a.compound_id = s.compound_id \n",
    "        AND a.protein_id = s.protein_id\n",
    "    WHERE ABS((a.ic50_nm - s.mean_ic50) / s.std_ic50) > 2.5\n",
    "        AND s.std_ic50 > 0\n",
    "    ORDER BY ABS((a.ic50_nm - s.mean_ic50) / s.std_ic50) DESC\n",
    "\"\"\"\n",
    "\n",
    "outliers = con.execute(query).df()\n",
    "print(f\"\\n\\nOutlier measurements (|z-score| > 2.5): {len(outliers)}\")\n",
    "print(outliers.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb97bd",
   "metadata": {},
   "source": [
    "We identified measurements that are more than 2.5 standard deviations away from the mean for their compound-protein pair. In a real analysis, we would investigate these outliers: are they experimental errors, or do they represent genuine biological variability?\n",
    "\n",
    "For this exercise, we will proceed by excluding extreme outliers from our analysis to focus on reliable data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6967e57",
   "metadata": {},
   "source": [
    "## Step 5: Focus on Cancer-Relevant Kinase Targets\n",
    "\n",
    "Our project manager specifically asked for cancer-relevant kinase inhibitors. Let's use SQL to join our tables and filter for relevant data, then calculate aggregate statistics for each compound-protein pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a5709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a clean dataset of cancer-relevant kinase assays\n",
    "query = \"\"\"\n",
    "    WITH reliable_measurements AS (\n",
    "        SELECT \n",
    "            compound_id,\n",
    "            protein_id,\n",
    "            AVG(ic50_nm) as mean_ic50,\n",
    "            STDDEV(ic50_nm) as std_ic50,\n",
    "            COUNT(*) as num_replicates,\n",
    "            AVG(efficacy_percent) as mean_efficacy\n",
    "        FROM assay_results_df\n",
    "        WHERE ic50_nm < 10000  -- Exclude extremely weak binders\n",
    "        GROUP BY compound_id, protein_id\n",
    "        HAVING COUNT(*) >= 2  -- Require at least 2 replicates\n",
    "    )\n",
    "    SELECT \n",
    "        c.compound_id,\n",
    "        c.compound_name,\n",
    "        c.molecular_weight,\n",
    "        c.log_p,\n",
    "        c.batch_quality,\n",
    "        p.protein_id,\n",
    "        p.protein_name,\n",
    "        p.protein_family,\n",
    "        p.disease_relevance,\n",
    "        rm.mean_ic50,\n",
    "        rm.std_ic50,\n",
    "        rm.num_replicates,\n",
    "        rm.mean_efficacy,\n",
    "        CASE \n",
    "            WHEN c.molecular_weight <= 500 \n",
    "                AND c.log_p <= 5 \n",
    "                AND c.h_bond_donors <= 5 \n",
    "                AND c.h_bond_acceptors <= 10 \n",
    "            THEN 1\n",
    "            ELSE 0\n",
    "        END as passes_lipinski\n",
    "    FROM reliable_measurements rm\n",
    "    INNER JOIN compounds_df c ON rm.compound_id = c.compound_id\n",
    "    INNER JOIN proteins_df p ON rm.protein_id = p.protein_id\n",
    "    WHERE p.protein_family = 'Kinase'\n",
    "        AND p.disease_relevance = 'Cancer'\n",
    "        AND c.batch_quality IN ('Excellent', 'Good')\n",
    "\"\"\"\n",
    "\n",
    "kinase_data = con.execute(query).df()\n",
    "\n",
    "print(f\"Cancer-relevant kinase data after quality filters:\")\n",
    "print(f\"  Total compound-protein pairs: {len(kinase_data):,}\")\n",
    "print(f\"  Unique compounds: {kinase_data['compound_id'].nunique():,}\")\n",
    "print(f\"  Unique proteins: {kinase_data['protein_id'].nunique():,}\")\n",
    "print(f\"  Compounds passing Lipinski: {kinase_data['passes_lipinski'].sum():,}\")\n",
    "\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(kinase_data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e1f642",
   "metadata": {},
   "source": [
    "We have successfully used SQL to perform complex filtering and joining across three tables, calculating aggregate statistics and applying multiple criteria. The result is a clean, focused dataset ready for detailed analysis in Pandas.\n",
    "\n",
    "Notice how SQL made this multi-step filtering process clear and efficient. The equivalent Pandas code would be more verbose and slower for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7439e0",
   "metadata": {},
   "source": [
    "## Step 6: Exploratory Data Analysis with Pandas\n",
    "\n",
    "Now that we have a focused dataset, let's use Pandas for exploratory analysis. We will examine the distribution of IC50 values, look for correlations between molecular properties and activity, and identify the most promising compounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd5e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IC50 Distribution Statistics:\")\n",
    "print(kinase_data['mean_ic50'].describe())\n",
    "\n",
    "print(\"\\n\\nEfficacy Distribution Statistics:\")\n",
    "print(kinase_data['mean_efficacy'].describe())\n",
    "\n",
    "kinase_data['potency_category'] = pd.cut(\n",
    "    kinase_data['mean_ic50'],\n",
    "    bins=[0, 10, 100, 1000, float('inf')],\n",
    "    labels=['Very Potent (<10 nM)', 'Potent (10-100 nM)', \n",
    "            'Moderate (100-1000 nM)', 'Weak (>1000 nM)']\n",
    ")\n",
    "\n",
    "print(\"\\n\\nDistribution by Potency Category:\")\n",
    "print(kinase_data['potency_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da14f50",
   "metadata": {},
   "source": [
    "The data shows that most compounds fall in the moderate to weak potency range, which is typical for early-stage screening. However, we do have some very potent compounds with IC50 values below 10 nM, which are excellent starting points for drug development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c029f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationship between molecular properties and potency\n",
    "# Focus on drug-like compounds\n",
    "druglike_kinase = kinase_data[kinase_data['passes_lipinski'] == 1].copy()\n",
    "\n",
    "# Add log-transformed IC50 for better visualization\n",
    "druglike_kinase['log_ic50'] = np.log10(druglike_kinase['mean_ic50'])\n",
    "\n",
    "print(f\"Drug-like kinase inhibitors: {len(druglike_kinase):,} compound-protein pairs\")\n",
    "print(f\"Unique drug-like compounds: {druglike_kinase['compound_id'].nunique():,}\")\n",
    "\n",
    "correlation_data = druglike_kinase[['molecular_weight', 'log_p', 'log_ic50', 'mean_efficacy']].corr()\n",
    "print(\"\\n\\nCorrelation Matrix:\")\n",
    "print(correlation_data.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e129125",
   "metadata": {},
   "source": [
    "The correlation analysis helps us understand relationships between molecular properties and biological activity. In drug discovery, we often look for compounds that balance potency (low IC50) with drug-like properties.\n",
    "\n",
    "A weak negative correlation between log P and log IC50 might suggest that more lipophilic compounds show better binding, but we need to be careful not to over-interpret small correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2ada28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify top candidates: most potent compounds with good properties\n",
    "top_candidates = (druglike_kinase\n",
    "    .groupby('compound_id')\n",
    "    .agg({\n",
    "        'compound_name': 'first',\n",
    "        'molecular_weight': 'first',\n",
    "        'log_p': 'first',\n",
    "        'mean_ic50': 'mean',  # Average across all kinases tested\n",
    "        'mean_efficacy': 'mean',\n",
    "        'protein_id': 'count',  # Number of kinases tested\n",
    "        'std_ic50': 'mean'  # Average variability\n",
    "    })\n",
    "    .rename(columns={'protein_id': 'num_kinases_tested'})\n",
    "    .sort_values('mean_ic50')\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "top_candidates['avg_cv'] = top_candidates['std_ic50'] / top_candidates['mean_ic50']\n",
    "\n",
    "print(\"Top 20 Most Potent Drug-Like Compounds:\")\n",
    "print(top_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f764fc",
   "metadata": {},
   "source": [
    "These top candidates combine strong potency with drug-like properties and have been tested against multiple kinase targets. Compounds tested against more targets give us confidence in their reproducibility and selectivity profile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eed9bf",
   "metadata": {},
   "source": [
    "## Step 7: Selectivity Analysis\n",
    "\n",
    "In drug discovery, selectivity is crucial. A compound that inhibits many kinases may cause side effects, while a highly selective compound may be safer. Let's analyze the selectivity profile of our top candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296ad8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each top compound, see how many kinases it potently inhibits\n",
    "selectivity_threshold = 100  # Consider IC50 < 100 nM as \"potent inhibition\"\n",
    "\n",
    "selectivity_analysis = []\n",
    "\n",
    "for compound_id in top_candidates.index[:10]:  # Analyze top 10 compounds\n",
    "    compound_data = kinase_data[kinase_data['compound_id'] == compound_id]\n",
    "    \n",
    "    total_tested = len(compound_data)\n",
    "    potent_hits = (compound_data['mean_ic50'] < selectivity_threshold).sum()\n",
    "    best_ic50 = compound_data['mean_ic50'].min()\n",
    "    worst_ic50 = compound_data['mean_ic50'].max()\n",
    "    \n",
    "    selectivity_analysis.append({\n",
    "        'compound_id': compound_id,\n",
    "        'compound_name': compound_data['compound_name'].iloc[0],\n",
    "        'kinases_tested': total_tested,\n",
    "        'potent_hits': potent_hits,\n",
    "        'selectivity_ratio': potent_hits / total_tested,\n",
    "        'best_ic50': best_ic50,\n",
    "        'worst_ic50': worst_ic50,\n",
    "        'dynamic_range': worst_ic50 / best_ic50\n",
    "    })\n",
    "\n",
    "selectivity_df = pd.DataFrame(selectivity_analysis).sort_values('selectivity_ratio')\n",
    "\n",
    "print(\"Selectivity Analysis (Top 10 Compounds):\")\n",
    "print(selectivity_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e368de",
   "metadata": {},
   "source": [
    "A compound with a low selectivity ratio (potently inhibits only a few of the kinases tested) is more selective, which is generally desirable to minimize off-target effects. The dynamic range (ratio of worst to best IC50) also indicates selectivity: a large range means the compound discriminates well between different kinases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b1d1a4",
   "metadata": {},
   "source": [
    "## Step 8: Visualization\n",
    "\n",
    "Let's create visualizations to present our findings. Good visualizations are essential for communicating results to non-technical stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097d81b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Cancer-Relevant Kinase Inhibitor Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Distribution of IC50 values (log scale)\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(np.log10(kinase_data['mean_ic50']), bins=40, edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(np.log10(100), color='red', linestyle='--', linewidth=2, label='100 nM threshold')\n",
    "ax1.set_xlabel('log10(IC50) in nM', fontsize=11)\n",
    "ax1.set_ylabel('Frequency', fontsize=11)\n",
    "ax1.set_title('Distribution of IC50 Values', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Molecular weight vs IC50 for drug-like compounds\n",
    "ax2 = axes[0, 1]\n",
    "scatter = ax2.scatter(\n",
    "    druglike_kinase['molecular_weight'], \n",
    "    druglike_kinase['log_ic50'],\n",
    "    c=druglike_kinase['mean_efficacy'],\n",
    "    cmap='viridis',\n",
    "    alpha=0.6,\n",
    "    s=30\n",
    ")\n",
    "ax2.set_xlabel('Molecular Weight (Da)', fontsize=11)\n",
    "ax2.set_ylabel('log10(IC50) in nM', fontsize=11)\n",
    "ax2.set_title('Molecular Weight vs Potency', fontsize=12, fontweight='bold')\n",
    "cbar = plt.colorbar(scatter, ax=ax2)\n",
    "cbar.set_label('Efficacy (%)', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Potency categories\n",
    "ax3 = axes[1, 0]\n",
    "potency_counts = kinase_data['potency_category'].value_counts().sort_index()\n",
    "colors_potency = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
    "ax3.bar(range(len(potency_counts)), potency_counts.values, color=colors_potency, edgecolor='black')\n",
    "ax3.set_xticks(range(len(potency_counts)))\n",
    "ax3.set_xticklabels(potency_counts.index, rotation=45, ha='right', fontsize=9)\n",
    "ax3.set_ylabel('Number of Compound-Protein Pairs', fontsize=11)\n",
    "ax3.set_title('Distribution by Potency Category', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Top 10 compounds by average IC50\n",
    "ax4 = axes[1, 1]\n",
    "top_10_plot = top_candidates.head(10).sort_values('mean_ic50', ascending=False)\n",
    "bars = ax4.barh(range(len(top_10_plot)), top_10_plot['mean_ic50'], color='#3498db', edgecolor='black')\n",
    "ax4.set_yticks(range(len(top_10_plot)))\n",
    "ax4.set_yticklabels([name.split('-')[1] for name in top_10_plot['compound_name']], fontsize=9)\n",
    "ax4.set_xlabel('Mean IC50 (nM)', fontsize=11)\n",
    "ax4.set_title('Top 10 Most Potent Compounds', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (idx, row) in enumerate(top_10_plot.iterrows()):\n",
    "    ax4.text(row['mean_ic50'] + 1, i, f\"{row['mean_ic50']:.1f}\", \n",
    "             va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualizations created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a10b544",
   "metadata": {},
   "source": [
    "We have identified a set of potent, drug-like kinase inhibitors with IC50 values well below 100 nM. The molecular weight versus potency plot shows that potent compounds can be found across the drug-like molecular weight range, and higher efficacy compounds are distributed throughout the potency spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84babc7f",
   "metadata": {},
   "source": [
    "## Step 9: Generate Final Report Using SQL + Pandas\n",
    "\n",
    "Let's create a final summary table that we can present. This will combine SQL for initial filtering and Pandas for final formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42cdd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL to get comprehensive statistics, then format in Pandas\n",
    "query = \"\"\"\n",
    "    WITH compound_summary AS (\n",
    "        SELECT \n",
    "            c.compound_id,\n",
    "            c.compound_name,\n",
    "            c.molecular_weight,\n",
    "            c.log_p,\n",
    "            AVG(a.mean_ic50) as avg_ic50_all_kinases,\n",
    "            MIN(a.mean_ic50) as best_ic50,\n",
    "            COUNT(DISTINCT a.protein_id) as num_kinases_tested,\n",
    "            AVG(a.mean_efficacy) as avg_efficacy,\n",
    "            SUM(CASE WHEN a.mean_ic50 < 100 THEN 1 ELSE 0 END) as potent_kinase_count\n",
    "        FROM kinase_data a\n",
    "        INNER JOIN compounds_df c ON a.compound_id = c.compound_id\n",
    "        WHERE a.passes_lipinski = 1\n",
    "        GROUP BY c.compound_id, c.compound_name, c.molecular_weight, c.log_p\n",
    "        HAVING COUNT(DISTINCT a.protein_id) >= 3  -- Tested against at least 3 kinases\n",
    "    )\n",
    "    SELECT \n",
    "        compound_id,\n",
    "        compound_name,\n",
    "        ROUND(molecular_weight, 1) as mw,\n",
    "        ROUND(log_p, 2) as logp,\n",
    "        ROUND(avg_ic50_all_kinases, 2) as avg_ic50,\n",
    "        ROUND(best_ic50, 2) as best_ic50,\n",
    "        num_kinases_tested,\n",
    "        ROUND(avg_efficacy, 1) as avg_efficacy,\n",
    "        potent_kinase_count,\n",
    "        ROUND(100.0 * potent_kinase_count / num_kinases_tested, 1) as selectivity_pct\n",
    "    FROM compound_summary\n",
    "    ORDER BY avg_ic50_all_kinases\n",
    "    LIMIT 15\n",
    "\"\"\"\n",
    "\n",
    "final_report = con.execute(query).df()\n",
    "\n",
    "final_report['priority_score'] = (\n",
    "    (1000 / final_report['avg_ic50']) * 0.5 +  # Potency weight: 50%\n",
    "    (final_report['avg_efficacy'] / 100) * 0.3 +  # Efficacy weight: 30%\n",
    "    ((100 - final_report['selectivity_pct']) / 100) * 0.2  # Selectivity weight: 20%\n",
    ")\n",
    "\n",
    "final_report['priority_rank'] = final_report['priority_score'].rank(ascending=False, method='dense').astype(int)\n",
    "final_report = final_report.sort_values('priority_rank')\n",
    "\n",
    "print(\"=\" * 100)\n",
    "print(\"FINAL REPORT: Top 15 Drug Candidates for Cancer-Relevant Kinase Inhibition\")\n",
    "print(\"=\" * 100)\n",
    "print(final_report.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b2cfc",
   "metadata": {},
   "source": [
    "## Report Summary\n",
    "\n",
    "The priority score combines three key factors with arbitray weights (let's assume this is just what our project manager has communicated to us):\n",
    "\n",
    "- Potency (50% weight): Lower IC50 values indicate stronger binding\n",
    "- Efficacy (30% weight): Higher efficacy suggests better therapeutic potential\n",
    "- Selectivity (20% weight): Lower selectivity percentage means the compound is more selective (fewer off-target kinases)\n",
    "\n",
    "The top-ranked compounds represent the best balance of these properties and should be prioritized for further development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca69b7b",
   "metadata": {},
   "source": [
    "## Step 10: Additional Analysis - Temporal Trends\n",
    "\n",
    "As a bonus analysis, let's examine whether compound synthesis and testing patterns have changed over time. This can reveal insights about research focus and experimental throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505425ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze testing activity over time using SQL\n",
    "query = \"\"\"\n",
    "    WITH monthly_activity AS (\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', a.assay_date) as test_month,\n",
    "            COUNT(*) as num_tests,\n",
    "            COUNT(DISTINCT a.compound_id) as unique_compounds,\n",
    "            AVG(a.mean_ic50) as avg_ic50_that_month\n",
    "        FROM kinase_data a\n",
    "        GROUP BY DATE_TRUNC('month', a.assay_date)\n",
    "    )\n",
    "    SELECT \n",
    "        test_month,\n",
    "        num_tests,\n",
    "        unique_compounds,\n",
    "        ROUND(avg_ic50_that_month, 2) as avg_ic50\n",
    "    FROM monthly_activity\n",
    "    ORDER BY test_month\n",
    "\"\"\"\n",
    "\n",
    "temporal_data = con.execute(query).df()\n",
    "temporal_data['test_month'] = pd.to_datetime(temporal_data['test_month'])\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "fig.suptitle('Temporal Trends in Kinase Testing', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot testing volume over time\n",
    "ax1 = axes[0]\n",
    "ax1.plot(temporal_data['test_month'], temporal_data['num_tests'], \n",
    "         marker='o', linewidth=2, markersize=6, color='#3498db')\n",
    "ax1.set_ylabel('Number of Tests', fontsize=11)\n",
    "ax1.set_title('Testing Activity Over Time', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot average IC50 over time\n",
    "ax2 = axes[1]\n",
    "ax2.plot(temporal_data['test_month'], temporal_data['avg_ic50'], \n",
    "         marker='s', linewidth=2, markersize=6, color='#e74c3c')\n",
    "ax2.set_ylabel('Average IC50 (nM)', fontsize=11)\n",
    "ax2.set_xlabel('Month', fontsize=11)\n",
    "ax2.set_title('Average Potency Over Time', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Temporal analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96401d92",
   "metadata": {},
   "source": [
    "This temporal analysis can reveal important patterns. For example, if average IC50 decreases over time, it suggests that the research team is successfully optimizing compounds or focusing on more promising chemical series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0afd80",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! In this hands-on session, we completed a full data analysis workflow combining SQL and Pandas:\n",
    "\n",
    "**Step 1-2**: We used SQL for initial data exploration and summary statistics, taking advantage of SQL's ability to quickly aggregate large datasets.\n",
    "\n",
    "**Step 3**: We applied Lipinski's Rule of Five using SQL's CASE statements to filter for drug-like compounds early in our pipeline.\n",
    "\n",
    "**Step 4**: We performed quality control by identifying high-variability compounds and outlier measurements using SQL's statistical functions and subqueries.\n",
    "\n",
    "**Step 5**: We combined multiple tables with INNER JOINs and applied complex filters to focus on cancer-relevant kinase inhibitors, demonstrating SQL's strength in relational data operations.\n",
    "\n",
    "**Step 6-7**: We switched to Pandas for detailed exploratory analysis, creating new features and performing selectivity analysis that would be more cumbersome in pure SQL.\n",
    "\n",
    "**Step 8-9**: We created visualizations and a final report, using SQL for the final data preparation and Pandas for ranking and formatting.\n",
    "\n",
    "**Step 10**: We demonstrated how SQL and Pandas can work together for temporal analysis.\n",
    "\n",
    "## When to Use SQL vs Pandas?\n",
    "\n",
    "The best approach combined both tools:\n",
    "- Use SQL when you need to filter large datasets before loading into memory, when joining multiple tables from a database, when performing aggregations that can be indexed efficiently in a database, or when writing self-documenting queries that colleagues can understand.\n",
    "- Use Pandas when performing complex transformations specific to Python, when integrating with Python visualization and statistical libraries, when working with data that is already in memory, or when iterative exploratory analysis requires flexibility.\n",
    "\n",
    "In this hybrid approach, we leveraged SQL's efficiency for data preparation, then used Pandas' flexibility for detailed analysis and visualization.\n",
    "\n",
    "Remember that the goal is not to use one tool exclusively, but to understand the strengths of each and combine them effectively. SQL excels at filtering and aggregating large datasets, while Pandas provides unmatched flexibility for exploratory analysis and integration with Python's scientific computing ecosystem.\n",
    "\n",
    "As you continue in your data science journey, you will develop intuition for when to reach for SQL versus Pandas bersus something else, and how to structure workflows that leverage multiple tools optimally."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
