{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "028ca453-21db-40c8-873c-082b36a3213a",
   "metadata": {},
   "source": [
    "# Feature Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56774547-98cb-469f-9fe9-6b080ab88a7c",
   "metadata": {},
   "source": [
    "## 1) Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b57b7b-813a-45c5-ab58-818f9a1ab636",
   "metadata": {},
   "source": [
    "Feature vectors usually contain a mixture of **categorical** and **numerical** features/variables. While numerical values can be normalized via scaling functions such as *StandardScaler* or *MinMaxScaler*, categorical features have to be encoded. There are four criteria that determine which encoding strategy is most suitable:<br> \n",
    "<br>\n",
    "- Are the feature values/states nominal (= **no inherent order** such as gene names, cities etc)?<br>\n",
    "- Are the feature values/states ordinal (= **there is an inherent order**) and the feature values can be ranked (health status, tax bracket etc)?<br>\n",
    "- cardinality $c$ of the feature (= how many states/values can a feature have)<br>\n",
    "- memory usage<br>\n",
    "<br>\n",
    "There are many encoding stategies. Here, we want to discuss the most common ones:<br>\n",
    "<br>\n",
    "- one-hot encoding<br>\n",
    "- dummy encoding<br>\n",
    "- ordinal encoding<br>\n",
    "- binary encoding<br>\n",
    "- count encoding<br>\n",
    "<br>\n",
    "Later, for example in Chem 277B, we will discuss more sophisticated encoding strategies like word embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a0726-753f-41a0-9109-b9c70200925a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3ca4b-1efc-4882-93cd-97703d2a1a4e",
   "metadata": {},
   "source": [
    "## 2) Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f779de07-dd51-45ca-9eff-05409db99bd2",
   "metadata": {},
   "source": [
    "First, we call the standard libraries as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff981c69-53bf-47a3-8b65-7d82107828dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3ff6f-9c36-45f2-b333-595713633626",
   "metadata": {},
   "source": [
    "Next, we load a dataset that contrains information of Alzheimer patients. The original source can be found [here](https://pubmed.ncbi.nlm.nih.gov/34233656/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17741dd0-bf00-4641-b02d-f137958abf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD = pd.read_excel('../Datasets/AD_data.xlsx', sheet_name = 'Summary Form')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57490494-10d9-451a-857a-aa0706629a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ea1fd-1e38-46e6-a938-da7141bbd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(AD.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea3fe6-8f16-428c-a771-dbc2f54f8fa5",
   "metadata": {},
   "source": [
    "Let us visualize the entire dataset in pairplot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4dd16-fedb-4db5-97f0-acea45d30d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pair = sns.pairplot(AD, kind = 'kde')\n",
    "Pair.map_lower(sns.scatterplot, c = 'k')\n",
    "Pair.map_upper(sns.scatterplot, c = 'k')\n",
    "Pair.map_diag(sns.histplot, stat = 'density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b7ebb-5209-495e-8e42-24af3cf66984",
   "metadata": {},
   "source": [
    "We immediately see that most of the features are **categorical**. We therefore check for **cardinality**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcaeafd-6e45-47cb-a92f-209a55319be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categoricals = AD.columns[:-2]\n",
    "for c in Categoricals:\n",
    "    card = len(set(AD[c]))\n",
    "    print(\"Cardinality of \" + c + ': '+ str(card))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339056fc-d0ee-4e2d-a4f7-833c92974b43",
   "metadata": {},
   "source": [
    "Fortunately, cardinality is low, but we need to distinguish between ordinal and nominal features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8237f-d531-48c5-8647-dd71ddbc8bcd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feee8fa0-e865-44ed-9508-08bb61577ea9",
   "metadata": {},
   "source": [
    "## 3) Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32576de-c05d-4e04-961f-74a478fdac5c",
   "metadata": {},
   "source": [
    "**3.1) One-hot**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacc15b8-5666-4210-b059-ea0e8b471b1b",
   "metadata": {},
   "source": [
    "The simplest encoding strategy is **one-hot encoding** that **assigns a vector** which only contains one binary value (1/True) at a certain position indicading the state, and another binary value (0/False) elsewhere. One-hot encoding is applicable if the feature values are nominal and if cardinality is low. For example, we can one-hot encode a DNA sequence (states are \"ACGT\" and $c=4$), which is done quite often in bioinformatics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c367cf51-4fa4-4e27-948a-88838f0b3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) defining keys and values for encoding via a dictionary\n",
    "Cat = \"ACGT\"                # keys\n",
    "Enc = np.identity(len(Cat)) # values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae70317-82af-4e95-a30b-76db1f637c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) actual encoder: a dictionary\n",
    "Encoding = {c: e for c, e in zip(Cat, Enc)}\n",
    "Encoder  = lambda Sequence: [Encoding[NT] for NT in Sequence] # defining a function via lambda that performs the encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43833014-344d-484d-a514-2c5bd75ef46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3) running an arbitrary test sequence:\n",
    "S = \"CCTGGTACACTATAGGCT\"\n",
    "print(np.array(Encoder(S)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2d4653-1e27-4090-a5bc-9a9b87a6c08b",
   "metadata": {},
   "source": [
    "The sequence is now encoded. Each nucleotide is encoded via an one-hot vector. The length of the vector equals the cardinality of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42459f-dd58-4839-918b-1d8d26ddf70a",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087554b8-3cba-4910-8a8c-abfa5177f704",
   "metadata": {},
   "source": [
    "**3.2) Dummy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04102c6d-4f0e-46fe-becf-495ad8f606bd",
   "metadata": {},
   "source": [
    "Instead of presenting a feature state by an entire vector, we can present the state by a **binary variable** (0/1 or False/True), which however comes with the downside of artificially adding new features (the \"dummies\") which represent the possible states of the given feature.<br>\n",
    "*Pandas* has a very convenient function for that, which is called *get_dummies*. Let's run this function for all categoricals in the Alzheimer dataset and try to understand what it does: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2435efbe-a121-4c1b-a75e-27cc1b44714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dummy = pd.get_dummies(AD, columns = Categoricals, dtype = bool)\n",
    "AD_dummy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208c9a8-2e6f-456c-883f-6b05ca0a4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dummy = pd.get_dummies(AD, columns = Categoricals, dtype = int)\n",
    "AD_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64a578-4abe-4c78-806e-695983ec6def",
   "metadata": {},
   "source": [
    "The features are dummy encoded now, but the information is redundant. For example, if we know that $state_0 = 1$, we also know that $state_1 = 0$. That is a perfect 100% anti-correlation which will lead some machine learning methods to fail or perform poorly. Therefore, we drop one column (usually the first) of each feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb464fc-8baa-4d0d-99c9-0fe494e90cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dummy = pd.get_dummies(AD, columns = Categoricals, dtype = int, drop_first = True)\n",
    "AD_dummy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e0a772-464b-4101-9ab6-9c5d1f4a4615",
   "metadata": {},
   "source": [
    "However, some of the features in the data are ordinal and have an inherent order. These features are $economic$, $education$, $age$ (which should not have been binned in the first place!), $health$ and $lifestyle$. For these features, another encoding strategy has to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ca0cd-71fb-4ead-bfbf-d1fcce1e558f",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e430fd-7ecc-40bc-bdae-7d5a670014bd",
   "metadata": {},
   "source": [
    "**3.3) Ordinal**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35bc489-426a-4a21-b309-80a09bacb746",
   "metadata": {},
   "source": [
    "Ordinal encoding is essentially enumerating the feature states according to their inherent order. That has been done already in the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1b4869-32fb-477f-9822-cfd388052a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD[['age range(0: <75; 1: >=75)', 'lifestyle', 'economic', 'education', 'health']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e7dfab-baa3-4c28-bff8-f19c894f326b",
   "metadata": {},
   "source": [
    "Hence, we leave these features as they are, but dummy encode those which are nominal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d898ac3c-1b20-42c4-9ccb-2ae6d5fe2079",
   "metadata": {},
   "outputs": [],
   "source": [
    "AD_dummy_proper = pd.get_dummies(AD, columns = ['sex', 'heridity', 'marriage',], dtype = int, drop_first = True)\n",
    "AD_dummy_proper.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe31373-379e-422a-8302-511b38bb03e0",
   "metadata": {},
   "source": [
    "**Important**:<br> \n",
    "<br>\n",
    "1) Now since the features have been encoded, we need to scale/normalize them as discussed in the lecture. Most of the features are not normally distributed and therefore, a max/min scaler is most suitable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1b33b5-d8b7-4ab4-be65-db937f1b8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c439a1c-4130-4da6-885e-5eaba79bdf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMinMax = MinMaxScaler(feature_range = (0, 1))                    # initializing the scaler\n",
    "Scaled  = SMinMax.fit_transform(AD_dummy_proper)                  # fit/transform the data\n",
    "Data    = pd.DataFrame(Scaled, columns = AD_dummy_proper.columns) # turning output into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7bcf7a-c150-49e4-8ea6-a4eab9e7e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c611074-7691-4115-8b7f-749550d1a2e7",
   "metadata": {},
   "source": [
    "Now, the dataset is ready for analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7fb43-1fe9-43f4-b59e-fdac5280971e",
   "metadata": {},
   "source": [
    "2) Encoding always implies bias, especially ordinal encoding. Try to avoid ordinal encoding as much as possible and use actual objective values instead. For example, instead of $health$, features like blood pressure, cholesterol etc contain more information and are less biased. The same applies for economic status (use annual income, savings, real estate values etc) and lifestyle (hours exercise per week, cigarette and alcohol consumption etc) and avoid vague features like *\"social status\"*, *\"anxiety level\"* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6762ace3-ba5c-4f87-98ba-df81a306ebe9",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d58881c-df79-48e0-91e2-3f4f998da6e9",
   "metadata": {},
   "source": [
    "**3.4) Binary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abaea09-2943-49c1-8f9c-a8e371e09a1a",
   "metadata": {},
   "source": [
    "A combination of ordinal and one-hot encoding is binary encoding, which is suitable if cardinality is high. We first assign an ordinal value to each state of the feature and then encode this number as binary, ususally 8-bit.<br> \n",
    "For example one-hot encoding a sentence is very inefficent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ffbda8-b365-4afe-95a0-93327f2e8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) building encoder as before\n",
    "Cat      = \"abcdefghijklmnopqrstuvwxyz .!?\"\n",
    "Enc      = np.identity(len(Cat))\n",
    "\n",
    "Encoding = {c: e for c, e in zip(Cat, Enc)}\n",
    "Encoder  = lambda Sentence: [Encoding[letter] for letter in Sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833cb36b-d3c9-476e-a9db-689b0873b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) encoding one sentence   \n",
    "My_Sentence = 'this is a sentence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819d718-6588-4624-a22a-75c444ec6e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(Encoder(My_Sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa3f18-3026-4b79-a797-d7a906ca0ec7",
   "metadata": {},
   "source": [
    "As we can see, storing this sentence in an one-hot encoded array is very memory inefficent, since cardinality $c$ is high. The sentence is stored in an array of shape $c\\,\\times\\,len(My\\_Sentence)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64187ca0-70b7-41d2-89b3-a9c9c3c81986",
   "metadata": {},
   "source": [
    "Let us encode the same sentence now using binary encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179d17f-d881-4445-8ab1-c5c802172dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) building encoder as before\n",
    "bytes_object \t= Cat.encode('utf-8') # first, the string has to be turned into a utf-8bit object\n",
    "Encoding     \t= {c: f\"{b:08b}\" for c, b in zip(Cat, bytes_object)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3013e9-9c73-4847-ade1-27f34f19acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder  = lambda Sentence: [Encoding[letter] for letter in Sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7d622-a07d-434f-baec-188e74a8f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(Encoder(My_Sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c372e-80e2-4164-88bb-3789b934284a",
   "metadata": {},
   "source": [
    "Apparently, the array is a lot smaller now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db95486d-4f15-485d-b09e-3c941d806a09",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c299228-e671-4db0-a919-46c4d93de319",
   "metadata": {},
   "source": [
    "**4.5) Count encoding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9895d-8ce7-4d2a-98ce-10244fd7db00",
   "metadata": {},
   "source": [
    "Count encoding or frequency encoding assigns the number of occurrences of a feature state/value to the encoded value. This is in particular helpful, if we want to derive quantities like information and specifity (i.e. calculating **entropy**) from those features. For example one can count how often a gene was overexpressed for certain disease types in medical records and therefore determine the specifity. That however only works for large datasets when interpreting the relative frequency of a value as probability is a good approximation (see sampling methods).<br>\n",
    "Let us run this encoding for the sentence as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82daf0dc-2c43-4a8b-84e8-f2d22cb991f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoding = {s: Sequence.count(s) for s in set(Sequence)} # counting occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc99818-66bd-4943-b5cf-cf4c7bfe95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder  = lambda Sentence: [Encoding[letter] for letter in Sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a08a948-7a45-4706-bb5c-a582f74a2d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(Encoder(My_Sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9d378a-00b5-43f2-8425-2f334ada7ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
