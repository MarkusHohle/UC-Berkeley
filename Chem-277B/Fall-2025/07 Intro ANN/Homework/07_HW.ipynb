{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af23393",
   "metadata": {},
   "source": [
    "# Homework Assignment 7 - Chem 277B\n",
    "## Neural Networks and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab7dec",
   "metadata": {},
   "source": [
    "### 1) Objective\n",
    "\n",
    "The goal is to perform different regression and classification tasks using neural networks and to compare the performance to standard tools such as linear regression. In order to understand how an ANN actually works, we want to use our custom layers (see lecture) for the analysis.<br>\n",
    "**Note:** in order to create your ANN efficiently, you can follow the structure provided in the lecture material (ANNI.ipynb, ANNII.ipynb and ANNIII.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb895c",
   "metadata": {},
   "source": [
    "### 2) Preparation\n",
    "\n",
    "Before starting, import the necessary libraries for data analysis and visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12b949-8c4d-4815-8992-bee0ab87a7e1",
   "metadata": {},
   "source": [
    "Next, we define our custom layers, such as the dense layer, activation functions and finally an optimizer and a loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bd0399",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "class Activation_Step():\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = (inputs >= 0).astype(float)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.zeros_like(dvalues)\n",
    "\n",
    "class Activation_ReLU():\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "class Activation_Sigmoid():\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.clip(1 / (1 + np.exp(-inputs)), 1e-7, 1 - 1e-7)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        sigm = self.output\n",
    "        deriv = sigm * (1 - sigm)\n",
    "        self.dinputs = deriv * dvalues\n",
    "\n",
    "class Activation_Softmax():\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # self.dinputs = np.empty_like(dvalues)\n",
    "        # for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "        #     single_output = single_output.reshape(-1, 1)\n",
    "        #     jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "        #     self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "        jacobian_matrices = (\n",
    "            np.einsum('ij,jk->ijk', self.output, np.eye(self.output.shape[1]))\n",
    "            - np.einsum('ij,ik->ijk', self.output, self.output)\n",
    "        )\n",
    "        self.dinputs = np.einsum('ijk,ik->ij', jacobian_matrices, dvalues)\n",
    "\n",
    "class Optimizer_SGD:\n",
    "\n",
    "    def __init__(self, learning_rate = 0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        weight_updates = -self.learning_rate * layer.dweights\n",
    "        bias_updates = -self.learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "class Loss_MeanSquaredError:\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.shape == y_true.shape, \"Shapes of predicted and true values must match.\"\n",
    "        return np.mean((y_pred - y_true) ** 2)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        Nsamples = len(dvalues)\n",
    "        self.dinputs = 2 * (dvalues - y_true) / Nsamples\n",
    "\n",
    "class Loss_BinaryCrossEntropy:\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.shape == y_true.shape, \"Shapes of predicted and true values must match.\"\n",
    "        correct_confidences = y_pred * y_true + (1 - y_pred) * (1 - y_true)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        Nsamples = len(dvalues)\n",
    "        self.dinputs = - (y_true / dvalues - (1 - y_true) / (1 - dvalues)) / Nsamples\n",
    "\n",
    "class Loss_MultiClassCrossEntropy:\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        assert y_pred.shape == y_true.shape, \"Shapes of predicted and true values must match.\"\n",
    "        correct_confidences = np.sum(y_pred * y_true, axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "    \n",
    "    def backward(self, dvalues, y_true):\n",
    "        Nsamples = len(dvalues)\n",
    "        self.dinputs = - (y_true / dvalues) / Nsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733dbdcd",
   "metadata": {},
   "source": [
    "### 3) Regression Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec91f269",
   "metadata": {},
   "source": [
    "#### 3.1) Data Generation\n",
    "\n",
    "First, we will perform a regression task using a neural network on a synthetic dataset generated from an exponential function with added noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e913d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.uniform(-2, 2, (100, 1))\n",
    "y = np.exp(x) + 0.1 * np.random.randn(len(x), 1)\n",
    "\n",
    "plt.plot(x, y, 'k.', label='Data points')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd35da",
   "metadata": {},
   "source": [
    "#### 3.2) Linear Model\n",
    "\n",
    "Without using a neural network, fit a linear regression model to the data and visualize the results. For that purpose, you can use a single dense layer without activation as the model (**recall: the neuron itself is just a linear model**), and use the gradient descent to minimize the mean squared error. Plot the original data points and the model predictions. Discuss the performance of the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d358510",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8699365a",
   "metadata": {},
   "source": [
    "*Your discussion*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5e754a",
   "metadata": {},
   "source": [
    "Some of the early activation functions, like a linear function or a Heaviside step function, are not used in modern neural networks. Explain why these activation functions are not suitable for training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb4536",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4146f0e0",
   "metadata": {},
   "source": [
    "#### 3.3) Single Neuron\n",
    "\n",
    "Use a single neuron to fit the same data. Again, train the network using gradient descent to minimize the mean squared error, and plot the original data points and the model predictions. Compare the performance of the single neuron model with that of the linear regression model.\n",
    "\n",
    "Hint: You need a linear layer to scale the input of the neuron, an activation function (e.g., ReLU, which I recommend for its simplicity and effectiveness), and another linear layer to scale the output of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89198624",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdacd5fd",
   "metadata": {},
   "source": [
    "*Your discussion*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368c7378",
   "metadata": {},
   "source": [
    "#### 3.4) Neural Network\n",
    "\n",
    "Now, use a neural network with 1 hidden layer containing 2 neurons and an appropriate activation function (e.g., ReLU) to fit the same data. Train the model, plot the original data points and the model predictions, and compare the performance with the previous models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635fe5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d280da3",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175e70e",
   "metadata": {},
   "source": [
    "#### 3.5) Universal Approximation Theorem\n",
    "\n",
    "According to the universal approximation theorem, a neural network with a single hidden layer containing a sufficient number of neurons can approximate any continuous function. Try a large number of neurons (e.g., 128 or 1024) in the hidden layer, and see how well the model fits the data. Discuss your observations. Does the theory hold in practice? If not, what could be the reasons and possible solutions? \n",
    "\n",
    "Hint: You don't need to implement the solutions, just discuss them. Or you can try them if you want in Question 6 afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13deb8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f258ae1",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf5d966",
   "metadata": {},
   "source": [
    "### 4) Binary Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8f245",
   "metadata": {},
   "source": [
    "#### 4.1) Data Generation\n",
    "\n",
    "The second task is a binary classification problem. Generate the double moon dataset using the provided function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b358b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = make_moons(n_samples=1000, noise=0.2, shuffle=True, random_state=42)\n",
    "x[:, 0] = x[:, 0] - 0.5\n",
    "x[:, 1] = x[:, 1] - 0.25\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=10)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325fa9ff",
   "metadata": {},
   "source": [
    "#### 4.2) Linear Model\n",
    "\n",
    "Using a single dense layer without activation as the model, train the network using gradient descent to minimize the binary cross-entropy loss. Plot the data points and the model predictions. Discuss the performance of the linear model.\n",
    "\n",
    "Hint: The model should output a single value between 0 and 1 to represent the probability of one of the classes. Use the sigmoid activation function after the dense layer to output probabilities between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b56faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################\n",
    "plt.contour(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.5)\n",
    "plt.contourf(xx, yy, zz, levels=np.linspace(0, 1, 33), alpha=0.2, zorder=-1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar(label='Model output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ecc48f",
   "metadata": {},
   "source": [
    "*Your discussion*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba8a54e",
   "metadata": {},
   "source": [
    "#### 4.3) Neural Network Model\n",
    "\n",
    "Using a neural network with 1 hidden layer, fit the double moon dataset. How many neurons are needed in the hidden layer? Discuss the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88617e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################\n",
    "plt.contour(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.5)\n",
    "plt.contourf(xx, yy, zz, levels=np.linspace(0, 1, 33), alpha=0.2, zorder=-1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar(label='Model output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad459de",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33406931",
   "metadata": {},
   "source": [
    "#### 4.4) Universal Approximation Theorem\n",
    "\n",
    "Instead of going wider with more neurons in the hidden layer, this time try adding more hidden layers to the network. Use more than 1 hidden layer with the same number of neurons as before for each layer and discuss the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################\n",
    "plt.contour(xx, yy, zz, levels=[0, 0.5, 1], alpha=0.5)\n",
    "plt.contourf(xx, yy, zz, levels=np.linspace(0, 1, 33), alpha=0.2, zorder=-1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar(label='Model output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333acbc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed078900",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9665bbd",
   "metadata": {},
   "source": [
    "### 5) Multiclass Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861aae1c",
   "metadata": {},
   "source": [
    "#### 5.1) Data Generation\n",
    "\n",
    "The last task is a multiclass classification problem. Combine two double moon datasets using the provided function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4fe872",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, y1 = make_moons(n_samples=500, noise=0.2, shuffle=True, random_state=42)\n",
    "x2, y2 = make_moons(n_samples=500, noise=0.2, shuffle=True, random_state=24)\n",
    "x1[:, 0] = x1[:, 0] - 0.5\n",
    "x1[:, 1] = x1[:, 1] + 0.75\n",
    "x2[:, 0] = x2[:, 0] - 0.5\n",
    "x2[:, 1] = x2[:, 1] - 1.25\n",
    "x = np.vstack([x1, x2])\n",
    "y = np.hstack([y1, y2 + 2])\n",
    "y_onehot = np.zeros((len(y), 4))\n",
    "y_onehot[np.arange(len(y)), y] = 1\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, s=10)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73f9b9",
   "metadata": {},
   "source": [
    "#### 5.2) Linear Model\n",
    "\n",
    "At this point, you should be familiar with the process. Using a single dense layer without activation as the model, train the network using gradient descent to minimize the multi-class cross-entropy loss. Plot the data points and the model predictions. Discuss the performance of the linear model.\n",
    "\n",
    "Hint: The model should output 4 values (one for each class) for each data point. Use the softmax activation function after the dense layer to convert the outputs into probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28df0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################\n",
    "plt.contourf(xx, yy, zz, levels=np.linspace(0, 4, 33), alpha=0.2, zorder=-1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar(label='Model output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa199ce0",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841170dc",
   "metadata": {},
   "source": [
    "#### 5.3) Neural Network Model\n",
    "\n",
    "Use a neural network to fit the multiclass dataset. How many layers and neurons are needed in the hidden layer? Discuss the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8064db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "########################################\n",
    "plt.contourf(xx, yy, zz, levels=np.linspace(0, 4, 33), alpha=0.2, zorder=-1)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.colorbar(label='Model output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7485d3",
   "metadata": {},
   "source": [
    "*Your discussion*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61eac5e",
   "metadata": {},
   "source": [
    "### 6) Moving Beyond (Optional)\n",
    "\n",
    "You might have noticed that simply increasing the number of neurons or layers does not always lead to better performance. The best way to improve the model is highly dependent on the specific problem and dataset, and often requires experimentation and tuning. Try out some of the following techniques to see if you can achieve better results on the multiclass classification task. \n",
    "\n",
    "- Model architecture changes\n",
    "    - Adding more layers (deepening the network)\n",
    "    - Adding more neurons (widening the network)\n",
    "    - Using different activation functions (e.g., sigmoid, tanh, Leaky ReLU)\n",
    "    - Changing the initialization of weights and biases (e.g., Kaiming initialization, Xavier initialization)\n",
    "- Loss function modifications\n",
    "    - Incorporating regularization techniques (e.g., L1, L2 regularization)\n",
    "- Optimization techniques\n",
    "    - Using advanced optimizers (e.g., Adam, RMSprop)\n",
    "    - Implementing learning rate schedules (e.g., step decay, exponential decay)\n",
    "- Data processing\n",
    "    - Normalizing or standardizing input features\n",
    "    - Normalizing or standardizing target values\n",
    "    - Batching the data for training\n",
    "\n",
    "Since you have the access to all the attributes of the layers, loss functions, and optimizers, feel free to modify them as needed. Document your findings and discuss the impact of these changes on the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc44ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x = np.random.uniform(-2, 2, (100, 1))\n",
    "y = np.exp(x) + 0.1 * np.random.randn(len(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9aa7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "######## Fill in the code below ########\n",
    "\n",
    "\n",
    "########################################\n",
    "xx = np.linspace(-2, 2, 1000).reshape(-1, 1)\n",
    "dense1.forward(xx)\n",
    "relu.forward(dense1.output)\n",
    "dense2.forward(relu.output)\n",
    "plt.plot(xx, dense2.output, 'r-', label='Model predictions')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf92441",
   "metadata": {},
   "source": [
    "*Your discussion*\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
