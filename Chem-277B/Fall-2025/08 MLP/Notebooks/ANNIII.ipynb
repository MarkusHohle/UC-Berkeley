{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfce43f8-4b39-487d-be55-cce6907a3528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aafc06e9-e354-4534-b8b7-cbb2b24c0096",
   "metadata": {},
   "source": [
    "## Building a fully functional ANN III"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f71041-e640-44d9-ae10-b8ee1ca495b5",
   "metadata": {},
   "source": [
    "Finally, this is the fully functional network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73d9ac-ae6e-4580-ba60-b6987ded86e2",
   "metadata": {},
   "source": [
    "**0) Loading and Preparing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f7e48-47fd-4b33-8e55-467fbdaeb8ab",
   "metadata": {},
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2375136-822f-464d-9cec-94b072310497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing the spiral data set \n",
    "#pip install nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25113b3-5738-4fa9-a876-66100c534c41",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5729ea-c3f8-470a-8763-04acc0fb881d",
   "metadata": {},
   "source": [
    "Importing the spiral dataset (Nsamples for each class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22523517-ea4d-49de-934d-115c4e11fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsample  = 300\n",
    "Nclasses = 5\n",
    "[X, Y]   = spiral_data(samples = Nsample, classes = Nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a13b41-ae4b-4d49-a8fa-f47a9a5034f1",
   "metadata": {},
   "source": [
    "Plotting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cf14b-fdf5-46e3-bd10-4a31aa9e8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(Nclasses):\n",
    "    idx   = np.argwhere(Y == n)[:,0]\n",
    "    xplot = X[idx,0]\n",
    "    yplot = X[idx,1]\n",
    "    plt.scatter(xplot, yplot, marker = '.', color = [1/(n+1), n/Nclasses, 2/(2*n + 3)], label = 'class ' + str(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a773f-a67f-4d7c-9a08-51eb1f4cae5d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31be37-2562-4c41-a42f-61e847596224",
   "metadata": {},
   "source": [
    "**1) Defining all the Parts we need for an ANN**<br>\n",
    "<br>\n",
    "We start with defining the dense layer, including backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d64f5e-81b8-4e19-b51a-2a90ea9734d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #gradients\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases  = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        self.dinputs  = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f4c3d-ca8b-4a81-9c94-1976b49fad6b",
   "metadata": {},
   "source": [
    "and as well as for the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a526a2-5985-4794-b640-24aa98363a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU():\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.maximum(0, inputs)\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0#ReLU derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c5ea0-af12-4c15-8a62-4452d6950a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid():\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.clip(1/(1 + np.exp(-inputs)), 1e-7, 1-1e-7)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        sigm         = self.output\n",
    "        deriv        = np.multiply(sigm, (1 - sigm))#inner derivative of sigmoid\n",
    "        self.dinputs = np.multiply(deriv, dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3bb4e-a89f-47ec-8856-5a4021683d93",
   "metadata": {},
   "source": [
    "The next layers are needed for calculating the probabilities for the different classes using the Boltzmann distribution (softmax) and the loss which is the cross entropy here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485c166-7cb8-48f1-a6c3-0202270ff1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "  \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        exp_values  = np.exp(inputs - np.max(inputs, axis = 1,\\\n",
    "                                      keepdims = True))#max in order to \n",
    "                                                       #prevent overflow\n",
    "        #normalizing probs (Boltzmann dist.)\n",
    "        probabilities = exp_values/np.sum(exp_values, axis = 1,\\\n",
    "                                      keepdims = True)  \n",
    "        self.output   = probabilities                                                \n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        for i, (single_output, single_dvalues) in \\\n",
    "            enumerate(zip(self.output, dvalues)):\n",
    "            \n",
    "            single_output   = single_output.reshape(-1,1)\n",
    "            jacobMatr       = np.diagflat(single_output) - \\\n",
    "                              np.dot(single_output, single_output.T)\n",
    "            self.dinputs[i] = np.dot(jacobMatr, single_dvalues)\n",
    "\n",
    "\n",
    "class Loss:\n",
    "     \n",
    "     def calculate(self, output, y):\n",
    "         \n",
    "         sample_losses = self.forward(output, y)\n",
    "         data_loss     = np.mean(sample_losses)\n",
    "         return(data_loss)\n",
    "    \n",
    "    \n",
    "class Loss_CategoricalCrossEntropy(Loss): \n",
    "\n",
    "     def forward(self, y_pred, y_true):\n",
    "         samples = len(y_pred)\n",
    "         #removing vals close to zero and one bco log and accuracy\n",
    "         y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "         \n",
    "         #now, depending on how classes are coded, we need to get the probs\n",
    "         if len(y_true.shape) == 1:#classes are encoded as [[1],[2],[2],[4]]\n",
    "             correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "         elif len(y_true.shape) == 2:#classes are encoded as\n",
    "                                    #[[1,0,0], [0,1,0], [0,1,0]]\n",
    "             correct_confidences = np.sum(y_pred_clipped*y_true, axis = 1)\n",
    "         #now: calculating actual losses\n",
    "         negative_log_likelihoods = -np.log(correct_confidences)\n",
    "         return(negative_log_likelihoods)\n",
    "         \n",
    "     def backward(self, dvalues, y_true):\n",
    "         Nsamples = len(dvalues)\n",
    "         Nlabels  = len(dvalues[0])\n",
    "         #turning labels into one-hot i. e. [[1,0,0], [0,1,0], [0,1,0]], if\n",
    "         #they are not\n",
    "         if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(Nlabels)[y_true]\n",
    "         #normalized gradient\n",
    "         self.dinputs = -y_true/dvalues/Nsamples\n",
    "\n",
    "\n",
    "\n",
    "#Creating a class as parent for softmax, loss and entropy classes. \n",
    "#Actually not neccessary, but saves code when building the ANN\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss       = Loss_CategoricalCrossEntropy()\n",
    "        \n",
    "    def forward(self, inputs, y_true):\n",
    "        self.activation.forward(inputs)\n",
    "        self.output = self.activation.output#the probabilities\n",
    "        #calculates and returns mean loss\n",
    "        return(self.loss.calculate(self.output, y_true))\n",
    "        \n",
    "    def backward(self, dvalues, y_true):\n",
    "        Nsamples = len(dvalues)\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis = 1)\n",
    "        self.dinputs = dvalues.copy()\n",
    "        #calculating normalized gradient\n",
    "        self.dinputs[range(Nsamples), y_true] -= 1\n",
    "        self.dinputs = self.dinputs/Nsamples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4ffbb-00b8-42a7-bf09-9b064b12e702",
   "metadata": {},
   "source": [
    "Finally, we need an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d6c6eb-e127-4aec-8ae5-65c2b1c95e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "    #initializing with a default learning rate of 0.01\n",
    "    def __init__(self, learning_rate = 0.01, decay = 0, momentum = 0):\n",
    "        self.learning_rate         = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay                 = decay\n",
    "        self.iterations            = 0\n",
    "        self.momentum              = momentum\n",
    "        \n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1/ (1 + self.decay*self.iterations))\n",
    "        \n",
    "    def update_params(self, layer):\n",
    "        \n",
    "        #if we use momentum\n",
    "        if self.momentum:\n",
    "            \n",
    "            #check if layer has attribute \"momentum\"\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums   = np.zeros_like(layer.biases)\n",
    "                \n",
    "            #now the momentum parts\n",
    "            weight_updates = self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            bias_updates = self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates   = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases  += bias_updates\n",
    "        \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248279e-90d6-41da-be06-fb24e1cba3b2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496638a8-fba2-4846-ba6f-9b84eaf84423",
   "metadata": {},
   "source": [
    "**2) The ANN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447e23a-d3cf-4bd3-b0e5-43cf9060e011",
   "metadata": {},
   "source": [
    "As before, we call the different instances of the layers and stack them togehter to a *serial ANN* and add the backpropagation part in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6b8ca-4aaa-4675-9945-55bfb1fb1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nneurons1     = 128\n",
    "Nsteps        = 20000 #number of iterations aka epochs\n",
    "learning_rate = 0.2\n",
    "decay         = 0.001\n",
    "momentum      = 0.8\n",
    "\n",
    "Nfeatures     = X.shape[1]\n",
    "\n",
    "dense1        = Layer_Dense(Nfeatures, Nneurons1)\n",
    "dense_cla     = Layer_Dense(Nneurons1, Nclasses) #for classification: Nclasses values for each data point\n",
    "\n",
    "optimizer     = Optimizer_SGD(learning_rate = learning_rate, decay = decay, momentum = momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669d02a-c0af-422d-ae07-9617840a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU            = Activation_ReLU()\n",
    "Sigm            = Activation_Sigmoid()\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea54b77-832c-4818-a4c2-ea37acefd9cf",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e0602-adbc-4349-8102-5cb3f8a8e4f0",
   "metadata": {},
   "source": [
    "The target vector is $Y$. We want to rename $Y$ in order to be consistent with our slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a49aa-0ff2-46b3-9eb7-0ec63247a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "Target = Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f4759-ecdf-4d7c-af79-c4061d9a2289",
   "metadata": {},
   "source": [
    "As before, we run the forward and beckward part, but now in a loop and including the optimizer and some plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39d651-2cef-432b-8b0b-ba95c91c6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Monitor = np.zeros((Nsteps,3)) #for storing loss, learning rate and accuracy\n",
    "\n",
    "for epoch in range(Nsteps):\n",
    "    #forward part:\n",
    "    dense1.forward(X)\n",
    "    ReLU.forward(dense1.output)\n",
    "    dense_cla.forward(ReLU.output)\n",
    "    loss = loss_activation.forward(dense_cla.output, Target)\n",
    "    \n",
    "    #comparing target to output\n",
    "    predictions = np.argmax(loss_activation.output, axis = 1)\n",
    "    accuracy    = np.mean(predictions == Target)\n",
    "    \n",
    "    #backward part:\n",
    "    loss_activation.backward(loss_activation.output, Target)\n",
    "\n",
    "    #loss_activation.dinputs = dL, i.e. the derivative of the loss, hence the outermost derivative\n",
    "    dense_cla.backward(loss_activation.dinputs)\n",
    "    ReLU.backward(dense_cla.dinputs)\n",
    "    dense1.backward(ReLU.dinputs)\n",
    "    \n",
    "    #finally, we apply gradient descent to all learnables\n",
    "    optimizer.pre_update_params()#decaying learning rate\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense_cla)\n",
    "    optimizer.post_update_params()#just increasing iteration by one\n",
    "            \n",
    "    Monitor[epoch,0] = accuracy\n",
    "    Monitor[epoch,1] = loss\n",
    "    Monitor[epoch,2] = optimizer.current_learning_rate\n",
    "            \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'accuracy: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'actual learning rate: {optimizer.current_learning_rate}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907bd4a-fbd8-4c7f-8627-9752ec517017",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f36a6-e94f-4c43-81e4-7da8e18d7395",
   "metadata": {},
   "source": [
    "**3) Evaluating the Fit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7468497-aac8-45c7-8087-4778d7a29f6e",
   "metadata": {},
   "source": [
    "a) the training process itself: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abab4c8-321e-4432-96f0-8e93d5be1e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "xplot = np.arange(Nsteps)\n",
    "\n",
    "fig1, ax1 = plt.subplots(3, 1, sharex = True)\n",
    "ax1[0].plot(xplot, 100*Monitor[:,0])\n",
    "ax1[0].set_ylabel('accuracy [%]')\n",
    "ax1[1].plot(xplot, Monitor[:,1])\n",
    "ax1[1].set_ylabel('loss')\n",
    "ax1[2].plot(xplot, Monitor[:,2])\n",
    "ax1[2].set_ylabel(r'$\\alpha$')\n",
    "ax1[2].set_xlabel('epoch')\n",
    "plt.xscale('log', base = 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470e2352-635f-437a-8a02-e125f843e625",
   "metadata": {},
   "source": [
    "b) cross entropy and confusion chart  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e357d2-ff18-4858-9368-96936853ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = loss_activation.output\n",
    "ClassLabs     = np.arange(Nclasses)\n",
    "\n",
    "fig2, ax2 = plt.subplots(Nclasses, 1, sharex = True)\n",
    "fig2.set_figheight(6)\n",
    "fig2.subplots_adjust(hspace = 0.5)\n",
    "fig2.suptitle('entropy')\n",
    "for L in ClassLabs:\n",
    "    idx = np.argwhere(Target == L)[:,0]\n",
    "    (value, where) = np.histogram(probabilities[idx,L],\\\n",
    "                                  bins = np.arange(0,1,0.01),\\\n",
    "                                  density = True)\n",
    "    w = 0.5*(where[1:] + where[:-1])\n",
    "    ax2[L].plot(w, value, 'k-')\n",
    "    ax2[L].set_ylabel('frequency')\n",
    "    ax2[L].set_title('class number ' + str(L))\n",
    "ax2[Nclasses-1].set_xlabel('probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4df04a1-0e49-4cb4-a744-33444bf6f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = np.argmax(probabilities, axis = 1)\n",
    "cm               = confusion_matrix(Target, predicted_labels)\n",
    "\n",
    "# Step 4: Plot confusion matrix\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues',\\\n",
    "            xticklabels = ClassLabs, yticklabels = ClassLabs)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cf825-8c2e-4d6c-8709-fdc54888c3ad",
   "metadata": {},
   "source": [
    "c) plotting the predicted labels/classes vs the true labels/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada27614-9e4d-4879-b090-fe9e7d4c050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(Nclasses):\n",
    "    idx    = np.argwhere(Y == n)[:,0]\n",
    "    xplot  = X[idx,0]\n",
    "    yplot  = X[idx,1]\n",
    "\n",
    "    idxp   = np.argwhere(predicted_labels == n)[:,0]\n",
    "    xplotp = X[idxp,0]\n",
    "    yplotp = X[idxp,1]\n",
    "    \n",
    "    plt.scatter(xplot,  yplot,  marker = '.', color = [1/(n+1), n/Nclasses, 2/(2*n + 3)], label = 'class ' + str(n))\n",
    "    plt.scatter(xplotp, yplotp, marker = 'o', color = [1/(n+1), n/Nclasses, 2/(2*n + 3)])\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6779402-27a7-479f-8e15-25293b8be6a2",
   "metadata": {},
   "source": [
    "Not bad for the minimal setup. Try different **numbers of neurons** for the input layer and also try different values for **momentum**, implement **L1 and L2** regularization and see how far you can improve the result! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fbad61-07cd-4bfd-909b-ef3176c8c505",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = np.log(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddc908-6571-45b7-b383-7709ab7a6e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
