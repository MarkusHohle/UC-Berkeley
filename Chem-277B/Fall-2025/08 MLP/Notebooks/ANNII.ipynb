{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafc06e9-e354-4534-b8b7-cbb2b24c0096",
   "metadata": {},
   "source": [
    "## Building a fully functional ANN II"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f71041-e640-44d9-ae10-b8ee1ca495b5",
   "metadata": {},
   "source": [
    "In this part we add the backpropagation part to the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e73d9ac-ae6e-4580-ba60-b6987ded86e2",
   "metadata": {},
   "source": [
    "**0) Loading and Preparing Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f7e48-47fd-4b33-8e55-467fbdaeb8ab",
   "metadata": {},
   "source": [
    "Importing libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2375136-822f-464d-9cec-94b072310497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#importing the spiral data set \n",
    "#pip install nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25113b3-5738-4fa9-a876-66100c534c41",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5729ea-c3f8-470a-8763-04acc0fb881d",
   "metadata": {},
   "source": [
    "Importing the spiral dataset (Nsamples for each class):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22523517-ea4d-49de-934d-115c4e11fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nsample  = 300\n",
    "Nclasses = 5\n",
    "[X, Y]   = spiral_data(samples = Nsample, classes = Nclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a13b41-ae4b-4d49-a8fa-f47a9a5034f1",
   "metadata": {},
   "source": [
    "Plotting the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58cf14b-fdf5-46e3-bd10-4a31aa9e8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(Nclasses):\n",
    "    idx   = np.argwhere(Y == n)[:,0]\n",
    "    xplot = X[idx,0]\n",
    "    yplot = X[idx,1]\n",
    "    plt.scatter(xplot, yplot, marker = '.', color = [1/(n+1), n/Nclasses, 2/(2*n + 3)], label = 'class ' + str(n))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284a773f-a67f-4d7c-9a08-51eb1f4cae5d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b31be37-2562-4c41-a42f-61e847596224",
   "metadata": {},
   "source": [
    "**1) Defining all the Parts we need for an ANN**<br>\n",
    "<br>\n",
    "We start with defining the dense layer again, now including backpropagation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d64f5e-81b8-4e19-b51a-2a90ea9734d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense():\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        #gradients\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases  = np.sum(dvalues, axis = 0, keepdims = True)\n",
    "        self.dinputs  = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f4c3d-ca8b-4a81-9c94-1976b49fad6b",
   "metadata": {},
   "source": [
    "and as well as for the activation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a526a2-5985-4794-b640-24aa98363a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU():\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        self.output  = np.maximum(0, inputs)\n",
    "        self.inputs  = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0#ReLU derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2c5ea0-af12-4c15-8a62-4452d6950a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Sigmoid():\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.clip(1/(1 + np.exp(-inputs)), 1e-7, 1-1e-7)\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        sigm         = self.output\n",
    "        deriv        = np.multiply(sigm, (1 - sigm))#inner derivative of sigmoid\n",
    "        self.dinputs = np.multiply(deriv, dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0248279e-90d6-41da-be06-fb24e1cba3b2",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496638a8-fba2-4846-ba6f-9b84eaf84423",
   "metadata": {},
   "source": [
    "**2) The ANN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447e23a-d3cf-4bd3-b0e5-43cf9060e011",
   "metadata": {},
   "source": [
    "In the next step, we call the different instances of the layers and stack them togehter to a *serial ANN* and add the backpropagation part in reverse order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6b8ca-4aaa-4675-9945-55bfb1fb1b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nneurons1 = 64\n",
    "Nfeatures = X.shape[1]\n",
    "\n",
    "dense1    = Layer_Dense(Nfeatures, Nneurons1)\n",
    "dense_reg = Layer_Dense(Nneurons1, 1)        #for regression: one value for each data point\n",
    "dense_cla = Layer_Dense(Nneurons1, Nclasses) #for classification: Nclasses values (later: probability for each class) for each data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669d02a-c0af-422d-ae07-9617840a0c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ReLU      = Activation_ReLU()\n",
    "Sigm      = Activation_Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea54b77-832c-4818-a4c2-ea37acefd9cf",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e0602-adbc-4349-8102-5cb3f8a8e4f0",
   "metadata": {},
   "source": [
    "The target vector is $Y$. We want to rename $Y$ in order to be consistent with our slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a49aa-0ff2-46b3-9eb7-0ec63247a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)\n",
    "Target = Y.reshape((len(Y),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9f4759-ecdf-4d7c-af79-c4061d9a2289",
   "metadata": {},
   "source": [
    "In the next step, we run the *forward* part as before and then compare the predicted output $Ypred$ to the target output. This leads to the outermost derivative, the change of the loss function which is $E = \\frac{1}{2}(Ypred - Target)^2$ here, hence the derivative is $dE = (Ypred - Target)$.<br>\n",
    "The derivative $dE$ is then fed into the *backward* part of the last layer. Within this layer, two things will happen:<br> \n",
    "- 1) $dE$ will be multiplied with the inner derivative of the layer and this product is the new outer derivative when being passed on to the next layer and so on\n",
    "- 2) if this layer has learnables (weights and biases), the changes $dw$ and $db$ will be calculated too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd39d651-2cef-432b-8b0b-ba95c91c6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.001\n",
    "\n",
    "#forward part:\n",
    "dense1.forward(X)\n",
    "ReLU.forward(dense1.output)\n",
    "dense_reg.forward(ReLU.output)\n",
    "\n",
    "#comparing target to output\n",
    "Ypred = dense_reg.output\n",
    "dE    = Ypred - Target #outermost derivative\n",
    "\n",
    "MSE   = np.sum(abs(dE))/(Nsample*Nclasses)\n",
    "print('MSE = ' + str(MSE))\n",
    "\n",
    "#backward part:\n",
    "dense_reg.backward(dE)\n",
    "ReLU.backward(dense_reg.dinputs)\n",
    "dense1.backward(ReLU.dinputs)\n",
    "\n",
    "#finally, we apply gradient descent\n",
    "dense_reg.weights -= alpha * dense_reg.dweights \n",
    "dense_reg.biases  -= alpha * dense_reg.dbiases\n",
    "\n",
    "dense1.weights    -= alpha * dense1.dweights \n",
    "dense1.biases     -= alpha * dense1.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907bd4a-fbd8-4c7f-8627-9752ec517017",
   "metadata": {},
   "source": [
    "Run the above cell repeatedly. The MSE might increase in the first steps, but should go down after a few iterations. Try the same with different activation functions and learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec071a0-7a87-4a23-823c-9ed3b739e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b77c3b-fcd3-4fd8-934c-5beb39672c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab2ca87-039e-4868-b364-631d56bf23a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.stairs(Ypred[:,0], label = 'prediction')\n",
    "plt.stairs(Y, label = 'target')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
