{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af23393",
   "metadata": {},
   "source": [
    "# Homework Assignment 5 - Chem 277B\n",
    "## Rosenbrock Function Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdab7dec",
   "metadata": {},
   "source": [
    "### 1) Objective\n",
    "\n",
    "Optimization methods such as **Gradient Descent** play a crucial role when training ANNs and are important for many other machine learning tools. In this assignment, we want you to get an idea of how different flavours of two main optimization algorithms, **Gradient Descent** and **Simulated Annealing**, work. For that purpose, find the minimum of the Rosenbrock function using gradient-free and gradient-based optimization methods and visualize the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb895c",
   "metadata": {},
   "source": [
    "### 2) Preparation\n",
    "\n",
    "Before starting, import the necessary libraries for optimization and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb84277",
   "metadata": {},
   "source": [
    "Define the Rosenbrock function \n",
    "\n",
    "$$f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$$\n",
    "\n",
    "Visualize the function with `plt.contourf` or `go.Surface` to understand its landscape within $-2 \\leq x_1 \\leq 2$ and $-1 \\leq x_2 \\leq 3$. Where is the global minimum? Mark it on the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4733587",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f76d68",
   "metadata": {},
   "source": [
    "Derive the gradient of the Rosenbrock function. Visualize the (negative) gradient field using `plt.quiver` or `go.Cone`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef272db",
   "metadata": {},
   "source": [
    "### 3) Simulated Annealing\n",
    "\n",
    "Suppose you only have access to the function value but not its gradient, simulated annealing is perhaps the most straightforward optimization method to implement. Therefore, as a warm-up exercise, implement the simulated annealing algorithm to find the minimum of the Rosenbrock function.\n",
    "\n",
    "The algorithm works as follows:\n",
    "1. Start from an initial point (e.g., (-1, 2)).\n",
    "2. Generate a new candidate point by adding a small random perturbation to the current point.\n",
    "3. Decide whether to accept the new point based on the Metropolis criterion:\n",
    "$$P_\\text{move} = \\begin{cases} 1 & \\text{if } \\Delta E < 0 \\\\ e^{-\\Delta E / T} & \\text{if } \\Delta E \\geq 0 \\end{cases}$$\n",
    "4. Start with a high temperature $T$ (e.g., 1.0)\n",
    "5. Optional Gradually cool down (e.g., by multiplying $T$ by 0.9).\n",
    "6. Repeat steps 2-5 for a sufficient number of iterations (e.g., 1000).\n",
    "\n",
    "Plot the trajectory of points visited during the optimization on top of the contour plot of the Rosenbrock function. Also plot the function value versus iteration number to show convergence.\n",
    "\n",
    "Discuss the results. How close did you get to the global minimum? How does the choice of temperature affect the optimization? How does the cooling affect the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb610b",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fill in the code below ########\n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8845e81-848e-44d8-835f-b4607c0a75cd",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14099cba",
   "metadata": {},
   "source": [
    "### 4) Gradient Descent\n",
    "\n",
    "If you have access to the gradient of the function, gradient descent is a more efficient optimization method, especially in high-dimensional spaces. This is also the basis for many advanced optimization algorithms. Let's implement the basic gradient descent algorithm to find the minimum of the Rosenbrock function.\n",
    "\n",
    "The algorithm works as follows:\n",
    "1. Start from an initial point (e.g., (1.5, 2.5)).\n",
    "2. Update the current point using the gradient $g$ with a fixed learning rate $\\eta$ (e.g., 0.001):\n",
    "$$x \\leftarrow x - \\eta \\, g(x)$$\n",
    "3. Repeat step 2 for a sufficient number of iterations (e.g., 1000).\n",
    "\n",
    "Plot the trajectory of points visited during the optimization on top of the contour plot of the Rosenbrock function. Also plot the function value versus iteration number to show convergence.\n",
    "\n",
    "Discuss the results. How close did you get to the global minimum? How does the choice of learning rate affect the optimization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47123fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fill in the code below ########\n",
    "\n",
    "########################################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca847578-76cd-4efb-8bea-0b1c38b8334f",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d08f560",
   "metadata": {},
   "source": [
    "### 5) Momentum Method\n",
    "\n",
    "You may find that the basic gradient descent method converges slowly. In practice, more advanced variants of gradient descent are often used, such as momentum. The momentum method updates the current point as follows:\n",
    "1. Initialize the velocity $v$ to 0.\n",
    "2. Update the velocity with a momentum factor $\\mu$ (e.g., 0.9) and the gradient $g$:\n",
    "$$v \\leftarrow \\mu v + g(x)$$\n",
    "3. Update the current point using the velocity $v$ and learning rate $\\eta$:\n",
    "$$x \\leftarrow x - \\eta v$$\n",
    "4. Repeat steps 2-3 for a sufficient number of iterations (e.g., 1000).\n",
    "Implement the momentum method and compare its performance with the basic gradient descent method. Plot the trajectories and function values for both methods on the same plots for comparison.\n",
    "\n",
    "Discuss the results. How does the momentum method improve convergence? How do the hyperparameters affect the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2527a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fill in the code below ########\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "18be76d8-2a12-40bb-8877-ee7f0c5218b5",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383b6234",
   "metadata": {},
   "source": [
    "### 6) More Advanced Methods\n",
    "\n",
    "Research (e.g. [Bishop](https://www.bishopbook.com/), chapter 7.2.3 - 7.3.3) and implement advanced optimization methods (e.g., RMSprop, Adam, etc.) to optimize the Rosenbrock function. Some of these methods may require different hyperparameters or configurations (e.g., higher learning rates, different forms of momentum terms). Compare its performance with the previous methods. Explain why (or why not) it performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f72847",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Fill in the code below ########\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "raw",
   "id": "37d9b677-f7c7-4938-b449-b910d22ecbb6",
   "metadata": {},
   "source": [
    "Your answer here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
