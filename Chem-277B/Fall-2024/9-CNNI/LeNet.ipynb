{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5514957-c4d8-4e49-844b-ecd41d6d57b0",
   "metadata": {},
   "source": [
    "## Building LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf6ca1b-c467-4e76-993e-115707d8743d",
   "metadata": {},
   "source": [
    "In this exercise we want to build LeNet (LeCun et al., 1989), the first fully functional CNN for **classification**, in order to understand the syntax and the structure of a sequential CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa224c48-2bf8-4d7c-91c1-7f659bb3225f",
   "metadata": {},
   "source": [
    "**0) Loading Libraries and Tools**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdd4fd-31ad-419b-81a7-e3e6eeb97fee",
   "metadata": {},
   "source": [
    "Importing Tensorflow/Keras libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280ed93-9269-4e19-ba5c-9393b5e16f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.layers import Dense, Flatten, Conv2D, AveragePooling2D\n",
    "from keras.utils  import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80bf5f-2a0d-4f87-a69a-f2093a466a2e",
   "metadata": {},
   "source": [
    "<br>\n",
    "Like in the discussion session, we want to analyse the MNIST data set, but this time, using a CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e37a21-23ad-4ddb-8703-3450328b6460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e3b4fa-a648-41fc-982e-c8c423057e21",
   "metadata": {},
   "source": [
    "<br>\n",
    "Finally, we also load the standard libraries for plotting and numeric operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a02e8-6598-4af1-bb15-972d1ddee54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f1d9ac-ccc6-431a-87b8-969a4a817c01",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1278615a-c548-4333-a993-3646d1e40853",
   "metadata": {},
   "source": [
    "**1) Loading the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163fa7df-4b92-4a9e-8d57-dcf626e292e7",
   "metadata": {},
   "source": [
    "Let us examine the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7311191-df5a-4956-8d03-a76821426415",
   "metadata": {},
   "outputs": [],
   "source": [
    "(Train_x, Train_y), (Test_x, Test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fb42ea-d153-4dfb-8bf1-8774e1712ef0",
   "metadata": {},
   "source": [
    "<br>\n",
    "The dataset comprises $60\\,000$ images for training and $10\\,000$ images for testing. Each image is 28x28 pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08d59dd-9877-4aad-80b2-f8bc08968290",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_x.shape)\n",
    "print(Test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68e32a-8f3a-4c60-a7fc-259d2696aedc",
   "metadata": {},
   "source": [
    "<br>\n",
    "All images are 8bit encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245fb2d-c149-4a2e-ada6-259cf64c9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(Train_x[0,:,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff86deb-c6e2-4604-adfe-780ac6c19e87",
   "metadata": {},
   "source": [
    "<br>\n",
    "and are 2D matrices, hence gray scale images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de34764-49ab-49dc-8cdb-6642d6d692c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_x[0,:,:].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a8332e-4060-4956-8c0e-f5d112583c81",
   "metadata": {},
   "source": [
    "<br>\n",
    "Of course, we would like to plot a few images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2506df-d358-4afc-a469-98f3932c42a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (10, 10))\n",
    "fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "\n",
    "for i in range(100):\n",
    "     ax = fig.add_subplot(10, 10, i + 1, xticks = [], yticks = [])\n",
    "     ax.imshow(-Test_x[i,:,:] + np.max(Test_x[0,:,:]), cmap = 'gray', interpolation = 'nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345c381-9b99-48cc-9953-c9b340699cc7",
   "metadata": {},
   "source": [
    "<br>\n",
    "We also want to take a quick look at *Train_y*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aefdd3f-ca26-4be7-b8ba-91d5078af249",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2e7cf-072d-4d73-8551-1f5161670ddd",
   "metadata": {},
   "source": [
    "<br>\n",
    "In order to keep our coding clean and neat, we define *classes* for each step of our analysis, including loading and preprocessing the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbcec4-485f-420f-9318-5905bc477a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CallData:\n",
    "    \n",
    "    def CallMNIST():\n",
    "        \n",
    "        ######################################################################################################################################################\n",
    "        #loading and preparing training data\n",
    "        (Train_x, Train_y), (Test_x, Test_y) = mnist.load_data()\n",
    "\n",
    "        #adding new axis (to fit input shape required for ANN) and normalization (8 bit)\n",
    "        Train_X3D = Train_x[:, :, :, np.newaxis]/255\n",
    "        Test_X3D  = Test_x[:, :, :, np.newaxis]/255\n",
    "\n",
    "        #y has to be turned in actual categorical, otherwise Python will prompt the error message:\n",
    "        #    \"Shapes (none, 1) and (none, 10) are incompatible categorical_crossentropy\"\n",
    "\n",
    "        num_classes = np.max(Test_y) + 1\n",
    "\n",
    "        Train_y = to_categorical(Train_y, num_classes)\n",
    "        Test_y  = to_categorical(Test_y, num_classes)\n",
    "\n",
    "        #check\n",
    "        Train_X3D.shape\n",
    "\n",
    "        ######################################################################################################################################################\n",
    "        #Splitting the training set into the actual training set and an evaluation set we want to process during the training in order to compare\n",
    "        #training loss to evaluation loss.\n",
    "        #note: the evaluation to training ratio can also be implemented when calling model.fit using the keyword validation_split = 0.2 etc\n",
    "  \n",
    "        N = len(Train_y)\n",
    "\n",
    "        Train_X3D = np.array(Train_X3D)\n",
    "\n",
    "        idx     = random.sample(range(N), round(N*0.2))\n",
    "        val_x   = Train_X3D[idx, :, :, :]\n",
    "        train_x = Train_X3D[[i for i in range(N) if i not in idx], :, :]\n",
    "        val_y   = Train_y[idx]\n",
    "        train_y = Train_y[[i for i in range(N) if i not in idx]]\n",
    "\n",
    "        ######################################################################################################################################################\n",
    "\n",
    "        return(num_classes, train_x, val_x, train_y, val_y, Test_X3D, Test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9668f7-15d0-4bb9-bf70-3141e525aefb",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a1dd1-1665-48e9-8f86-8f4c33220946",
   "metadata": {},
   "source": [
    "**2) Building the Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904d925b-b42e-459d-8daa-3df5340986c9",
   "metadata": {},
   "source": [
    "Next, we want to generate a sequential CNN like LeNet. Most of the layers and keywords are familiar to us already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4affcdc-2366-4e40-9ef6-75a8d01c9ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLeNet(Sequential):\n",
    "    \n",
    "    def __init__(self, input_shape, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "#building LeNet ##############################################################################################################################################\n",
    "        #Note padding: string, either \"valid\" or \"same\" (case-insensitive). \"valid\" means no padding. \"same\" \n",
    "        #more info: https://keras.io/api/layers/convolution_layers/convolution2d/\n",
    "        \n",
    "        self.add(Conv2D(6,   kernel_size = (5, 5),    strides = (1, 1), activation = 'tanh', input_shape = input_shape, padding = 'same'))\n",
    "        self.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2),                                                 padding = 'valid'))\n",
    "        self.add(Conv2D(16,  kernel_size = (5, 5),    strides = (1, 1), activation = 'tanh',                            padding = 'valid'))\n",
    "        self.add(AveragePooling2D(pool_size = (2, 2), strides = (2, 2),                                                 padding = 'valid'))\n",
    "        self.add(Conv2D(120, kernel_size = (5, 5),    strides = (3, 3), activation = 'tanh',                            padding = 'valid'))\n",
    "        self.add(Flatten())\n",
    "        self.add(Dense(84,                                              activation = 'tanh'))\n",
    "        self.add(Dense(num_classes,                                     activation = 'softmax'))\n",
    "        ######################################################################################################################################################      \n",
    "\n",
    "#building the optimizer ######################################################################################################################################\n",
    "        \n",
    "        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate = 1e-2, decay_steps = 10000, decay_rate = 0.98)\n",
    "        opt         = tf.keras.optimizers.SGD(learning_rate = lr_schedule, momentum = 0.9)\n",
    "            \n",
    "        self.compile(optimizer = opt, loss = categorical_crossentropy, metrics = ['accuracy'])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1f398-f3c9-435a-b14d-284b25be6396",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55babcf-057c-4075-8a30-99b8dfef21c4",
   "metadata": {},
   "source": [
    "**3) Traning and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d29d29-fd48-4a7a-a266-ef6164553805",
   "metadata": {},
   "source": [
    "Finally, we want to run our network and evaluate the training. Also for that purpose, we define our own class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7472f178-fb55-4f8a-b6c7-453c16654181",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        [self.num_classes, self.train_x, self.val_x, self.train_y, self.val_y, self.Test_X3D, self.Test_y] = CallData.CallMNIST()\n",
    "        self.model                                                                                         = MyLeNet(self.train_x.shape[1:], self.num_classes)\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "    def Run(self, epochs = 20, batch_size = 512):\n",
    "\n",
    "        print('running model...')\n",
    "        history = self.model.fit(self.train_x, self.train_y, epochs = epochs, batch_size = batch_size, validation_data = (self.val_x, self.val_y), verbose = 1)\n",
    "\n",
    "    #plotting ################################################################################################################################################\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc = 'upper left')\n",
    "        plt.savefig('training results.pdf')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'val'], loc = 'upper left')\n",
    "        plt.savefig('training loss.pdf')\n",
    "        plt.show()\n",
    "    ##########################################################################################################################################################\n",
    "\n",
    "    def Eval(self):\n",
    "\n",
    "        predict = self.model.predict(self.Test_X3D)\n",
    "\n",
    "        fig = plt.figure(figsize = (7, 7))\n",
    "        fig.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0.05, wspace = 0.05)\n",
    "\n",
    "        \n",
    "        N   = 49 #picking 49 images randomly\n",
    "        idx = random.sample(range(self.Test_y.shape[0]), N)\n",
    "\n",
    "        # plot the images: each image is 28x28 pixels\n",
    "        for i in range(N):\n",
    "            ii = idx[i]\n",
    "            ax = fig.add_subplot(7, 7, i + 1, xticks = [], yticks = [])\n",
    "            ax.imshow(self.Test_X3D[ii,:,:,0].reshape((28,28)), cmap = plt.cm.gray_r, interpolation = 'nearest')\n",
    "            \n",
    "            predclass = np.argmax(predict[ii,:])\n",
    "            trueclass = np.argmax(self.Test_y[ii,:])\n",
    "            \n",
    "            S = str(predclass)\n",
    "          \n",
    "            if predclass == trueclass:\n",
    "                P = str(round(predict[ii, predclass], 2))            #probability\n",
    "                ax.text(0, 3, S + ', P = ' + P, color = [0, 1, 0.2])#label the image with the green text\n",
    "            else:\n",
    "                ax.text(0, 3, S, color = [1, 0 , 0.2])              #label the image with the red text\n",
    "                \n",
    "        plt.savefig('evaluation results.pdf')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15d543d-9a9a-4291-a228-bca230c1f418",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, our package is ready to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d446a1f0-40d9-4f80-a735-31b9213fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialization\n",
    "M = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb24f7c-d24f-4a84-8292-4c881a2a2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#running the model\n",
    "M.Run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de70aa9-18d9-4ed6-9bcc-93a59501c920",
   "metadata": {},
   "outputs": [],
   "source": [
    "M.Eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
