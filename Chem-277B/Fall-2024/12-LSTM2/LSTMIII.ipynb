{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c618c3-a50b-4b7a-8611-043670640ec0",
   "metadata": {},
   "source": [
    "## Different LSTM Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c89d13-fad7-443e-81fb-e0894b443594",
   "metadata": {},
   "source": [
    "We want to explore the different LSTM structures and evaluate the performance based on a simple, artificial dataset. The structures we are going to implement are:<br>\n",
    "<br>\n",
    "    - Vanilla LSTM: which is the standard LSTM we know already for comparison<br>\n",
    "    - Bidirectional LSTM<br>\n",
    "    - Stacked LSTM<br>\n",
    "    - LSTM + CNN<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bdad6-9657-4d5c-b96d-7d888e59c5ef",
   "metadata": {},
   "source": [
    "**0) Loading Libraries and Subroutines**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98acc91f-f3da-4c05-be20-ce803b1c564f",
   "metadata": {},
   "source": [
    "Standard libraries for plotting and numerical operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ab194-3b21-45c7-8387-aafd39ad3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90050234-9bfe-46aa-8d54-7226e6bef06e",
   "metadata": {},
   "source": [
    "Loading LSTM related keras libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245f694-3281-4e77-98bc-3641fca8aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be23df-b658-4560-9f9f-0b95486bf5b1",
   "metadata": {},
   "source": [
    "Calling a subroutine that puts the data set in the correct shape for LSTM (see later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce62fba-0ff4-4b93-bdcf-576610421807",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prepare_data import prepare_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca37431-93b4-44ee-a0e8-d475c5d80dc6",
   "metadata": {},
   "source": [
    "<br>\n",
    "As before, we generate a simple dataset, but this time with a higher noise level in order to challange the LSTMs we are going to build:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483fb87-1d0b-4712-944c-242a184b71e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = -50\n",
    "t_end   = 20\n",
    "incr    = 0.25\n",
    "\n",
    "t       = np.arange(t_start, t_end, incr)\n",
    "t       = t.reshape(len(t), 1)\n",
    "Y_t     = np.sin(t) + 0.5*np.random.randn(len(t), 1) + np.exp((t + 20)*0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9402644-1811-48c5-a41c-1169fdd21391",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, Y_t)\n",
    "plt.title('complete series')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3260bff7-10f9-4c19-9c84-93922ba33f96",
   "metadata": {},
   "source": [
    "a) Scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd712503-a991-470d-a174-fb34cb8ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler  = MinMaxScaler(feature_range = (0, 1))\n",
    "Y_tnorm = scaler.fit_transform(Y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319aa0b1-4350-4fc9-ab99-a12fc1f7d107",
   "metadata": {},
   "source": [
    "2b) Reshaping the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ce64a-55bd-4c2e-bce4-375ed1d810f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_past    = 20\n",
    "dt_futu    = 8\n",
    "n_features = 1\n",
    "\n",
    "[X, Y] = prepare_data(Y_tnorm, dt_past, dt_futu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b71849-bc60-46c5-8351-e65acf5c8787",
   "metadata": {},
   "source": [
    "2c) Splitting data into Training and Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e0c719-bff7-4e41-a18c-9a226120b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut            = int(np.round(0.7*Y_tnorm.shape[0]))\n",
    "\n",
    "TrainX, TrainY = X[:cut], Y[:cut]\n",
    "TestX,   TestY = X[cut:], Y[cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000836d4-7d77-4820-98f9-de445c3487a1",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bb035-9d4d-4afa-aacc-c637e80dc731",
   "metadata": {},
   "source": [
    "**1) Vanilla LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c0dfce-5e95-4632-867f-70479c82c2cc",
   "metadata": {},
   "source": [
    "As in the previous lecture, we start withe standard, aka *vanilla* LSTM<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96dedf5-df5e-4fde-b413-1ed36ddfcda6",
   "metadata": {},
   "source": [
    "1a) Generating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ab735e-3b3a-4560-94a9-13ac117280f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons  = 400\n",
    "batch_size = 128\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(n_neurons, input_shape = (dt_past, n_features), activation = 'tanh'))\n",
    "model.add(Dense(dt_futu))\n",
    "\n",
    "opt = optimizers.Adam()\n",
    "model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c657ee60-4a60-44ee-8bcd-c31592309138",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416285f-f739-41b8-aacf-a0c64b95dff3",
   "metadata": {},
   "source": [
    "1b) Fitting the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8cf7a-0619-4107-85cf-4a89c197fe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "out = model.fit(TrainX, TrainY, epochs = n_epochs, batch_size = batch_size, validation_split = 0.2, verbose = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87c15e-58fa-4d82-9532-0b5893ea734f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting #############################################################\n",
    "plt.plot(out.history['loss'])\n",
    "plt.plot(out.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc = 'upper left')\n",
    "plt.savefig('training loss.pdf')\n",
    "plt.show()\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea4f73-4732-406d-af20-da678e62a51d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5055ad5-85e1-4f6c-8e06-b99b117e4f38",
   "metadata": {},
   "source": [
    "1c) Evaluating the Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03886a-1d4d-4ce1-8c9e-db63db83a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredY = model.predict(TestX)\n",
    "back  = PredY.shape[0]\n",
    "\n",
    "plt.plot(t, Y_tnorm, linewidth = 3)\n",
    "plt.plot(t[-back:], PredY[:, dt_futu-1])\n",
    "plt.legend(['actual data', 'prediction'])\n",
    "plt.fill_between([t[-back,0], t[-1,0]], 0, 1, color = 'k', alpha = 0.1)\n",
    "plt.plot([t[-back,0], t[-back,0]], [0, 1], 'k-', linewidth = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b252c40-d2dd-4158-ac2f-f10af6b27b18",
   "metadata": {},
   "source": [
    "Let us run the same analysis, but with different LSTMs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8a9f6f-4f2b-4af5-ab31-a1c0a61414cd",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b95fc2-a750-43fd-b3e5-c37f662d7b2f",
   "metadata": {},
   "source": [
    "**2) Bidirectional LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10776ab8-da92-4c07-b041-fb7ac444352c",
   "metadata": {},
   "source": [
    "For many sequences (like i.e. DNA, RNA, AA) it makes sense to read them from both directions and therefore makes it easier to detect pattern. For example a pattern in DNA sense (ATTCA) and antisense (ACTTA) direction might look mirrowed, hence diffferent, but they are actually the same feature with the same function.<br>\n",
    "The only thing we need to do is call the corresponding library in *Keras*:<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e2502-172a-402a-b086-6d6d9b30379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767ca4fa-a1bf-49d2-8454-6f347d764e73",
   "metadata": {},
   "source": [
    "...and add the class to our model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d8e32-8a32-4b25-9680-52681bdb9943",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(n_neurons, activation = 'tanh'), input_shape = (dt_past, n_features)))\n",
    "model.add(Dense(dt_futu))\n",
    "\n",
    "opt = optimizers.Adam()\n",
    "model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc6d671-c2ca-4a5c-9e41-4ee4c71a698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.fit(TrainX, TrainY, epochs = n_epochs, batch_size = batch_size, validation_split = 0.2, verbose = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f589cfd1-51e4-4bf2-b07c-ef4f41f55111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting #############################################################\n",
    "plt.plot(out.history['loss'])\n",
    "plt.plot(out.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc = 'upper left')\n",
    "plt.savefig('training loss.pdf')\n",
    "plt.show()\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c14b34-633a-45f5-825e-b94d2b997463",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredY = model.predict(TestX)\n",
    "back  = PredY.shape[0]\n",
    "\n",
    "plt.plot(t, Y_tnorm, linewidth = 3)\n",
    "plt.plot(t[-back:], PredY[:, dt_futu-1])\n",
    "plt.legend(['actual data', 'prediction'])\n",
    "plt.fill_between([t[-back,0], t[-1,0]], 0, 1, color = 'k', alpha = 0.1)\n",
    "plt.plot([t[-back,0], t[-back,0]], [0, 1], 'k-', linewidth = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fccb6-b052-4e7b-a963-37a2a059631d",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4921aee9-1725-4fc7-a11d-8adef1aa5236",
   "metadata": {},
   "source": [
    "**3) Stacked LSTM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece0d787-b522-4070-afd5-45b0bda1d987",
   "metadata": {},
   "source": [
    "In the same way we can run different convolution layer subsequently, we can add different LSTMs as stacks.<br>\n",
    "For the **first LSTM**, we still need to provide the input shape. As an additional setting, we need to add *return_sequences = True* to **all LSTMs except the last one**, so that the output has the shape *(batch size i.e. sequence length x timesteps i.e. dt_past x hidden state)* in order to pass it on to the next LSTM layer (see matrix multiplication \"MLP\" lecture and \"LSTM1\" lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74d9b49-4226-4514-86db-602ba41f09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(n_neurons,   activation = 'tanh', return_sequences = True, input_shape = (dt_past, n_features)))\n",
    "model.add(LSTM(2*n_neurons, activation = 'relu', return_sequences = True))\n",
    "model.add(LSTM(n_neurons,   activation = 'relu'))\n",
    "model.add(Dense(dt_futu))\n",
    "\n",
    "opt = optimizers.Adam()\n",
    "model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376bfa96-3200-49d3-a02d-ff06ec2089eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.fit(TrainX, TrainY, epochs = n_epochs, batch_size = batch_size, validation_split = 0.2, verbose = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0420e-89c3-4fcd-b3a0-eb3aab1fcc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting #############################################################\n",
    "plt.plot(out.history['loss'])\n",
    "plt.plot(out.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc = 'upper left')\n",
    "plt.savefig('training loss.pdf')\n",
    "plt.show()\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8185d796-d555-4f1c-af28-757c3f50e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredY = model.predict(TestX)\n",
    "back  = PredY.shape[0]\n",
    "\n",
    "plt.plot(t, Y_tnorm, linewidth = 3)\n",
    "plt.plot(t[-back:], PredY[:, dt_futu-1])\n",
    "plt.legend(['actual data', 'prediction'])\n",
    "plt.fill_between([t[-back,0], t[-1,0]], 0, 1, color = 'k', alpha = 0.1)\n",
    "plt.plot([t[-back,0], t[-back,0]], [0, 1], 'k-', linewidth = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68242d2-148b-4a27-8b1a-83346bad42b8",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fedadb7-3afa-4e06-a90f-1670edf029ac",
   "metadata": {},
   "source": [
    "**4) LSTM + CNN**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad04702-96c5-435f-ac91-813a265d7acb",
   "metadata": {},
   "source": [
    "Both, CNN and LSTM are quite sucessful on detecting pattern. A logical step is to combine both structures with their strenghts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec3b51-3d28-4e42-a151-8136c58494f4",
   "metadata": {},
   "source": [
    "First, we need to call the corresponding libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f025674-48e2-4212-9552-d2064317d300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Conv1D, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78f86e2-0db6-4e8e-9078-7b13db74a7d3",
   "metadata": {},
   "source": [
    "But now there is a tricky part: The convolution layer expects the shape *(N_images, N_pixel_x, N_pixel_y, N_color_chan)*, but a sequence usually has the shape *(N_samples, N_timesteps, N_features)*. Thus, we first need to reshape the input matrix. But we also want to maintain the order of time. If we want to learn a pattern in time form a certain number of samples having a certain number of features, each time point needs to have the information from all features and all samples. Thus, the first coordinate is time (see the lecture slides for more details).<br> \n",
    "Thererfore, the shape for $X$ has to be *(N_timesteps, N_samples, dt_past, N_features)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6e4062-b6f6-4c1f-93df-7eacc33c13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples  = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48728289-e4cf-4dd2-9bf7-71a175cbe992",
   "metadata": {},
   "outputs": [],
   "source": [
    "X          = X.reshape((X.shape[0], N_samples, dt_past, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093bf18-9111-41f9-add2-875b01161c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainX, TrainY = X[:cut], Y[:cut]\n",
    "TestX,   TestY = X[cut:], Y[cut:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0848f5ad-1ca5-47d2-a869-08716a215c9b",
   "metadata": {},
   "source": [
    "The next step is to make sure that the shapes from the convolution filters are passed on to the LSTM in the correct way. This is done by using the wrapper *TimeDistributed*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be7890-4548-4fb5-81aa-3ecf6b51293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f2fe1-5e87-4cfd-bce9-53045f4587f8",
   "metadata": {},
   "source": [
    "Now, we are ready for building the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f82d3-91a9-435b-86cc-4af4cd444533",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv1D(filters = 64, kernel_size = 3, activation = 'relu'), input_shape = (None, dt_past, n_features)))\n",
    "model.add(TimeDistributed(MaxPooling1D(pool_size = 2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(LSTM(n_neurons, input_shape = (dt_past, n_features), activation = 'tanh'))\n",
    "model.add(Dense(dt_futu))\n",
    "\n",
    "opt = optimizers.Adam()\n",
    "model.compile(loss = 'mean_squared_error', optimizer = opt)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8537a6f-07ba-47dc-bf29-1cfaf90bf6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.fit(TrainX, TrainY, epochs = n_epochs, batch_size = batch_size, validation_split = 0.2, verbose = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f92c59-b101-4fe9-beef-a84eab11ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting #############################################################\n",
    "plt.plot(out.history['loss'])\n",
    "plt.plot(out.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc = 'upper left')\n",
    "plt.savefig('training loss.pdf')\n",
    "plt.show()\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1063d-b1d9-4d8f-8ce7-34e481fb6faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PredY = model.predict(TestX)\n",
    "back  = PredY.shape[0]\n",
    "\n",
    "plt.plot(t, Y_tnorm, linewidth = 3)\n",
    "plt.plot(t[-back:], PredY[:, dt_futu-1])\n",
    "plt.legend(['actual data', 'prediction'])\n",
    "plt.fill_between([t[-back,0], t[-1,0]], 0, 1, color = 'k', alpha = 0.1)\n",
    "plt.plot([t[-back,0], t[-back,0]], [0, 1], 'k-', linewidth = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca21b35b-e117-438b-a4ad-6202a69b196c",
   "metadata": {},
   "source": [
    "Compared to the other architectures, the result has improved alot!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
